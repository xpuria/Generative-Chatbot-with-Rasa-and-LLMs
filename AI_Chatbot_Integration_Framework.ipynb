{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq3YM-Q5SwNn"
      },
      "source": [
        "# **Deep Natural Language Processing @ PoliTO**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhUhU368S2Y-"
      },
      "source": [
        "# Simple chatbot architecture using RASA\n",
        "\n",
        "Chatbots can be defined as a computer program that simulates a conversation with a human user. They are used in a wide range of applications, from customer service to e-commerce. In this practice, we will explore the RASA framework and build a simple chatbot that can answer some questions in specific domains.\n",
        "\n",
        "The goal of the practice is to explore the usage of intents, stories and domain definitions to add specific properties to our chatbot.\n",
        "\n",
        "The following cells install the RASA framework and set up the environment to start working on the practice. Please run them before starting the practice and restart the runtime when asked (there is a comment in the cell indicating when to restart)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipython\n",
        "!pip install nest_asyncio\n",
        "!pip install -U rasa"
      ],
      "metadata": {
        "id": "k2bHznox1eZi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d6573ac-ed03-4d78-f231-88d62560d9cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting rasa\n",
            "  Downloading rasa-3.6.20-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting CacheControl<0.13.0,>=0.12.9 (from rasa)\n",
            "  Downloading CacheControl-0.12.14-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from PyJWT[crypto]<3.0.0,>=2.0.0->rasa) (2.10.0)\n",
            "Collecting SQLAlchemy<1.5.0,>=1.4.0 (from rasa)\n",
            "  Downloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: absl-py<1.5,>=0.9 in /usr/local/lib/python3.10/dist-packages (from rasa) (1.4.0)\n",
            "Collecting aio-pika<8.2.4,>=6.7.1 (from rasa)\n",
            "  Downloading aio_pika-8.2.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting aiogram<2.26 (from rasa)\n",
            "  Downloading aiogram-2.25.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting aiohttp<3.10,>=3.9.0 (from rasa)\n",
            "  Downloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
            "Collecting apscheduler<3.10,>=3.6 (from rasa)\n",
            "  Downloading APScheduler-3.9.1.post1-py2.py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting attrs<22.2,>=19.3 (from rasa)\n",
            "  Downloading attrs-22.1.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting boto3<2.0.0,>=1.26.136 (from rasa)\n",
            "  Downloading boto3-1.35.69-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from rasa) (2024.8.30)\n",
            "Collecting cloudpickle<2.3,>=1.2 (from rasa)\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting colorclass<2.3,>=2.2 (from rasa)\n",
            "  Downloading colorclass-2.2.2-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting coloredlogs<16,>=10 (from rasa)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting colorhash<1.3.0,>=1.0.2 (from rasa)\n",
            "  Downloading colorhash-1.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting confluent-kafka<3.0.0,>=1.9.2 (from rasa)\n",
            "  Downloading confluent_kafka-2.6.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=41.0.7 in /usr/local/lib/python3.10/dist-packages (from rasa) (43.0.3)\n",
            "Collecting dask==2022.10.2 (from rasa)\n",
            "  Downloading dask-2022.10.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dnspython==2.3.0 (from rasa)\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting fbmessenger<6.1.0,>=6.0.0 (from rasa)\n",
            "  Downloading fbmessenger-6.0.0-py2.py3-none-any.whl.metadata (869 bytes)\n",
            "Requirement already satisfied: google-auth<3 in /usr/local/lib/python3.10/dist-packages (from rasa) (2.27.0)\n",
            "Collecting joblib<1.3.0,>=0.15.1 (from rasa)\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting jsonpickle<3.1,>=1.3 (from rasa)\n",
            "  Downloading jsonpickle-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting jsonschema<4.18,>=3.2 (from rasa)\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting matplotlib<3.6,>=3.1 (from rasa)\n",
            "  Downloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting mattermostwrapper<2.3,>=2.2 (from rasa)\n",
            "  Downloading mattermostwrapper-2.2.tar.gz (2.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting networkx<2.7,>=2.4 (from rasa)\n",
            "  Downloading networkx-2.6.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numpy<1.25.0,>=1.19.2 (from rasa)\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting packaging<21.0,>=20.0 (from rasa)\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from rasa) (1.5.0)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from rasa)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting prompt-toolkit<3.0.29,>=3.0 (from rasa)\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting protobuf<4.23.4,>=4.23.3 (from rasa)\n",
            "  Downloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
            "Collecting psycopg2-binary<2.10.0,>=2.8.2 (from rasa)\n",
            "  Downloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting pydantic<1.10.10 (from rasa)\n",
            "  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydot<1.5,>=1.4 (from rasa)\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pykwalify<1.9,>=1.7 (from rasa)\n",
            "  Downloading pykwalify-1.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pymongo<4.4,>=3.8 (from pymongo[srv,tls]<4.4,>=3.8->rasa)\n",
            "  Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: python-dateutil<2.9,>=2.8 in /usr/local/lib/python3.10/dist-packages (from rasa) (2.8.2)\n",
            "Collecting python-engineio!=5.0.0,<6,>=4 (from rasa)\n",
            "  Downloading python_engineio-4.10.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting python-socketio<6,>=4.4 (from rasa)\n",
            "  Downloading python_socketio-5.11.4-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pytz<2023.0,>=2019.1 (from rasa)\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: pyyaml>=6.0 in /usr/local/lib/python3.10/dist-packages (from rasa) (6.0.2)\n",
            "Collecting questionary<1.11.0,>=1.5.1 (from rasa)\n",
            "  Downloading questionary-1.10.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting randomname<0.2.0,>=0.1.5 (from rasa)\n",
            "  Downloading randomname-0.1.5.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting rasa-sdk<3.7.0,>=3.6.2 (from rasa)\n",
            "  Downloading rasa_sdk-3.6.2-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting redis<5.0,>=4.5.3 (from rasa)\n",
            "  Downloading redis-4.6.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting regex<2022.11,>=2020.6 (from rasa)\n",
            "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.23 in /usr/local/lib/python3.10/dist-packages (from rasa) (2.32.3)\n",
            "Collecting rocketchat_API<1.31.0,>=0.6.31 (from rasa)\n",
            "  Downloading rocketchat_API-1.30.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting ruamel.yaml<0.17.22,>=0.16.5 (from rasa)\n",
            "  Downloading ruamel.yaml-0.17.21-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting sanic<21.13,>=21.12 (from rasa)\n",
            "  Downloading sanic-21.12.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting sanic-cors<2.1.0,>=2.0.0 (from rasa)\n",
            "  Downloading Sanic_Cors-2.0.1-py2.py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting sanic-jwt<2.0.0,>=1.6.0 (from rasa)\n",
            "  Downloading sanic_jwt-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sanic-routing<0.8.0,>=0.7.2 (from rasa)\n",
            "  Downloading sanic_routing-0.7.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting scikit-learn<1.2,>=0.22 (from rasa)\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting scipy<1.11.0,>=1.10.0 (from rasa)\n",
            "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk<1.15.0,>=0.17.0 (from rasa)\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from rasa) (75.1.0)\n",
            "Collecting sklearn-crfsuite<0.4,>=0.3 (from rasa)\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting slack-sdk<4.0.0,>=3.19.2 (from rasa)\n",
            "  Downloading slack_sdk-3.33.4-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting structlog<24.0.0,>=23.1.0 (from rasa)\n",
            "  Downloading structlog-23.3.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting structlog-sentry<3.0.0,>=2.0.2 (from rasa)\n",
            "  Downloading structlog_sentry-2.2.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting tarsafe<0.0.5,>=0.0.3 (from rasa)\n",
            "  Downloading tarsafe-0.0.4-py3-none-any.whl.metadata (858 bytes)\n",
            "Collecting tensorflow==2.12.0 (from rasa)\n",
            "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.32 (from rasa)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (14 kB)\n",
            "Collecting tensorflow-text==2.12.0 (from rasa)\n",
            "  Downloading tensorflow_text-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting tensorflow_hub<0.14.0,>=0.13.0 (from rasa)\n",
            "  Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting terminaltables<3.2.0,>=3.1.0 (from rasa)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.31 in /usr/local/lib/python3.10/dist-packages (from rasa) (4.66.6)\n",
            "Collecting twilio<8.3,>=6.26 (from rasa)\n",
            "  Downloading twilio-8.2.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from rasa) (4.12.2)\n",
            "Collecting typing-utils<0.2.0,>=0.1.0 (from rasa)\n",
            "  Downloading typing_utils-0.1.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting ujson<6.0,>=1.35 (from rasa)\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting webexteamssdk<1.7.0,>=1.1.1 (from rasa)\n",
            "  Downloading webexteamssdk-1.6.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting websockets<11.0,>=10.0 (from rasa)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: wheel>=0.38.1 in /usr/local/lib/python3.10/dist-packages (from rasa) (0.45.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from dask==2022.10.2->rasa) (8.1.7)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from dask==2022.10.2->rasa) (2024.10.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from dask==2022.10.2->rasa) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.10/dist-packages (from dask==2022.10.2->rasa) (0.12.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (24.3.25)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (1.68.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (3.12.1)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (0.4.33)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (18.1.1)\n",
            "Collecting numpy<1.25.0,>=1.19.2 (from rasa)\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (3.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (1.16.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.12.0->rasa) (2.5.0)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting aiormq~=6.4.0 (from aio-pika<8.2.4,>=6.7.1->rasa)\n",
            "  Downloading aiormq-6.4.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: yarl in /usr/local/lib/python3.10/dist-packages (from aio-pika<8.2.4,>=6.7.1->rasa) (1.17.2)\n",
            "INFO: pip is looking at multiple versions of aiogram to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting aiogram<2.26 (from rasa)\n",
            "  Downloading aiogram-2.25.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.25-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.24-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.23.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.23-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.22.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "  Downloading aiogram-2.22.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "INFO: pip is still looking at multiple versions of aiogram to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading aiogram-2.22-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading aiogram-2.21-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading aiogram-2.20-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading aiogram-2.19-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading aiogram-2.18-py3-none-any.whl.metadata (3.5 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading aiogram-2.17.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading aiogram-2.17-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading aiogram-2.16-py3-none-any.whl.metadata (3.5 kB)\n",
            "  Downloading aiogram-2.15-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: Babel>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from aiogram<2.26->rasa) (2.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.0->rasa) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.0->rasa) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.0->rasa) (6.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<3.10,>=3.9.0->rasa) (4.0.3)\n",
            "Requirement already satisfied: tzlocal!=3.*,>=2.0 in /usr/local/lib/python3.10/dist-packages (from apscheduler<3.10,>=3.6->rasa) (5.2)\n",
            "Collecting botocore<1.36.0,>=1.35.69 (from boto3<2.0.0,>=1.26.136->rasa)\n",
            "  Downloading botocore-1.35.69-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0.0,>=1.26.136->rasa)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0.0,>=1.26.136->rasa)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from CacheControl<0.13.0,>=0.12.9->rasa) (1.1.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs<16,>=10->rasa)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=41.0.7->rasa) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3->rasa) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3->rasa) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3->rasa) (4.9)\n",
            "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema<4.18,>=3.2->rasa)\n",
            "  Downloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.6,>=3.1->rasa) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.6,>=3.1->rasa) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.6,>=3.1->rasa) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.6,>=3.1->rasa) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.6,>=3.1->rasa) (3.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.0.29,>=3.0->rasa) (0.2.13)\n",
            "Collecting docopt>=0.6.2 (from pykwalify<1.9,>=1.7->rasa)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting simple-websocket>=0.10.0 (from python-engineio!=5.0.0,<6,>=4->rasa)\n",
            "  Downloading simple_websocket-1.1.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting bidict>=0.21.0 (from python-socketio<6,>=4.4->rasa)\n",
            "  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting fire (from randomname<0.2.0,>=0.1.5->rasa)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.23->rasa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.23->rasa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.23->rasa) (2.2.3)\n",
            "Collecting ruamel.yaml.clib>=0.2.6 (from ruamel.yaml<0.17.22,>=0.16.5->rasa)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting httptools>=0.0.10 (from sanic<21.13,>=21.12->rasa)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting aiofiles>=0.6.0 (from sanic<21.13,>=21.12->rasa)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<3.10,>=3.9.0->rasa)\n",
            "  Downloading multidict-5.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting uvloop>=0.5.3 (from sanic<21.13,>=21.12->rasa)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2,>=0.22->rasa) (3.5.0)\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn-crfsuite<0.4,>=0.3->rasa)\n",
            "  Downloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite<0.4,>=0.3->rasa) (0.9.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<1.5.0,>=1.4.0->rasa) (3.1.1)\n",
            "INFO: pip is looking at multiple versions of structlog-sentry to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting structlog-sentry<3.0.0,>=2.0.2 (from rasa)\n",
            "  Downloading structlog_sentry-2.2.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "  Downloading structlog_sentry-2.1.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting aiohttp-retry>=2.8.3 (from twilio<8.3,>=6.26->rasa)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from webexteamssdk<1.7.0,>=1.1.1->rasa) (1.0.0)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from webexteamssdk<1.7.0,>=1.1.1->rasa) (1.0.0)\n",
            "Collecting pamqp==3.2.1 (from aiormq~=6.4.0->aio-pika<8.2.4,>=6.7.1->rasa)\n",
            "  Downloading pamqp-3.2.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=41.0.7->rasa) (2.22)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0->rasa) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow==2.12.0->rasa) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0->rasa)\n",
            "  Downloading jaxlib-0.4.35-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0->rasa)\n",
            "  Downloading jaxlib-0.4.34-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0->rasa)\n",
            "  Downloading jaxlib-0.4.31-cp310-cp310-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0->rasa)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0->rasa)\n",
            "  Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=0.3.10->dask==2022.10.2->rasa) (1.0.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3->rasa) (0.6.1)\n",
            "Collecting wsproto (from simple-websocket>=0.10.0->python-engineio!=5.0.0,<6,>=4->rasa)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (3.1.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl->aio-pika<8.2.4,>=6.7.1->rasa) (0.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto->simple-websocket>=0.10.0->python-engineio!=5.0.0,<6,>=4->rasa) (0.14.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->rasa) (3.2.2)\n",
            "Downloading rasa-3.6.20-py3-none-any.whl (838 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.7/838.7 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask-2022.10.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aio_pika-8.2.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiogram-2.15-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.2/184.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading APScheduler-3.9.1.post1-py2.py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.69-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading CacheControl-0.12.14-py2.py3-none-any.whl (21 kB)\n",
            "Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorhash-1.2.1-py3-none-any.whl (5.7 kB)\n",
            "Downloading confluent_kafka-2.6.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fbmessenger-6.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpickle-3.0.4-py3-none-any.whl (39 kB)\n",
            "Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.2/380.2 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.23.3-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Downloading pykwalify-1.8.0-py2.py3-none-any.whl (24 kB)\n",
            "Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_engineio-4.10.1-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_socketio-5.11.4-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading questionary-1.10.0-py3-none-any.whl (31 kB)\n",
            "Downloading rasa_sdk-3.6.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rocketchat_API-1.30.0-py3-none-any.whl (21 kB)\n",
            "Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sanic-21.12.2-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Sanic_Cors-2.0.1-py2.py3-none-any.whl (17 kB)\n",
            "Downloading sanic_jwt-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading sanic_routing-0.7.2-py3-none-any.whl (23 kB)\n",
            "Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Downloading slack_sdk-3.33.4-py2.py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading SQLAlchemy-1.4.54-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-23.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog_sentry-2.1.0-py3-none-any.whl (11 kB)\n",
            "Downloading tarsafe-0.0.4-py3-none-any.whl (5.3 kB)\n",
            "Downloading tensorflow_hub-0.13.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.6/100.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Downloading twilio-8.2.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
            "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webexteamssdk-1.6.1-py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.5/113.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading aiormq-6.4.2-py3-none-any.whl (34 kB)\n",
            "Downloading pamqp-3.2.1-py2.py3-none-any.whl (33 kB)\n",
            "Downloading bidict-0.23.1-py3-none-any.whl (32 kB)\n",
            "Downloading botocore-1.35.69-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multidict-5.2.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyrsistent-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.7/117.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_crfsuite-0.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (722 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.2/722.2 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simple_websocket-1.1.0-py3-none-any.whl (13 kB)\n",
            "Downloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp310-cp310-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: mattermostwrapper, randomname, docopt, fire\n",
            "  Building wheel for mattermostwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mattermostwrapper: filename=mattermostwrapper-2.2-py3-none-any.whl size=2447 sha256=c649b7c06398c872873f59d39b79b064030c7c6d533e6b2befc31d0c495d00c6\n",
            "  Stored in directory: /root/.cache/pip/wheels/b2/dc/02/e3239f0ea0a676085826846d32ef09b10915c0a33c817d2dbb\n",
            "  Building wheel for randomname (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for randomname: filename=randomname-0.1.5-py3-none-any.whl size=58808 sha256=196903f1a0ab1e0d0523af002cc9467eba2fbc28d2781d99ce676dae21c260ef\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/df/c1/0bdf17c694217f49657ca36fd0239b9a243c34c27a70ff56ef\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=106ed21a6ab7bee9cd613fa1135b83f1cab504a58c48355e34c319c22f6147e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=9e6940d2072c76312a09647e75069e6f306c04ca1db7ec8762a6171f71c6d5fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\n",
            "Successfully built mattermostwrapper randomname docopt fire\n",
            "Installing collected packages: sanic-routing, pytz, docopt, wsproto, wrapt, websockets, uvloop, ujson, typing-utils, terminaltables, tensorflow-io-gcs-filesystem, tensorflow-estimator, tarsafe, structlog, SQLAlchemy, slack-sdk, sentry-sdk, sanic-jwt, ruamel.yaml.clib, regex, redis, python-crfsuite, pyrsistent, pydot, pydantic, psycopg2-binary, protobuf, prompt-toolkit, portalocker, pamqp, packaging, numpy, networkx, multidict, keras, jsonpickle, joblib, jmespath, humanfriendly, httptools, gast, fire, dnspython, confluent-kafka, colorhash, colorclass, cloudpickle, bidict, attrs, apscheduler, aiofiles, tensorflow_hub, structlog-sentry, sklearn-crfsuite, simple-websocket, scipy, sanic, ruamel.yaml, rocketchat_API, randomname, questionary, pymongo, mattermostwrapper, matplotlib, jsonschema, fbmessenger, dask, coloredlogs, CacheControl, botocore, webexteamssdk, scikit-learn, sanic-cors, s3transfer, python-engineio, pykwalify, jaxlib, google-auth-oauthlib, aiormq, aiohttp, tensorboard, rasa-sdk, python-socketio, jax, boto3, aiohttp-retry, aiogram, aio-pika, twilio, tensorflow, tensorflow-text, rasa\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2024.2\n",
            "    Uninstalling pytz-2024.2:\n",
            "      Successfully uninstalled pytz-2024.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
            "    Found existing installation: tensorflow-io-gcs-filesystem 0.37.1\n",
            "    Uninstalling tensorflow-io-gcs-filesystem-0.37.1:\n",
            "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.37.1\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.36\n",
            "    Uninstalling SQLAlchemy-2.0.36:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.36\n",
            "  Attempting uninstall: sentry-sdk\n",
            "    Found existing installation: sentry-sdk 2.18.0\n",
            "    Uninstalling sentry-sdk-2.18.0:\n",
            "      Successfully uninstalled sentry-sdk-2.18.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.9.11\n",
            "    Uninstalling regex-2024.9.11:\n",
            "      Successfully uninstalled regex-2024.9.11\n",
            "  Attempting uninstall: pydot\n",
            "    Found existing installation: pydot 3.0.2\n",
            "    Uninstalling pydot-3.0.2:\n",
            "      Successfully uninstalled pydot-3.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.9.2\n",
            "    Uninstalling pydantic-2.9.2:\n",
            "      Successfully uninstalled pydantic-2.9.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt_toolkit 3.0.48\n",
            "    Uninstalling prompt_toolkit-3.0.48:\n",
            "      Successfully uninstalled prompt_toolkit-3.0.48\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "  Attempting uninstall: multidict\n",
            "    Found existing installation: multidict 6.1.0\n",
            "    Uninstalling multidict-6.1.0:\n",
            "      Successfully uninstalled multidict-6.1.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.5.0\n",
            "    Uninstalling keras-3.5.0:\n",
            "      Successfully uninstalled keras-3.5.0\n",
            "  Attempting uninstall: jsonpickle\n",
            "    Found existing installation: jsonpickle 4.0.0\n",
            "    Uninstalling jsonpickle-4.0.0:\n",
            "      Successfully uninstalled jsonpickle-4.0.0\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.4.2\n",
            "    Uninstalling joblib-1.4.2:\n",
            "      Successfully uninstalled joblib-1.4.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 3.1.0\n",
            "    Uninstalling cloudpickle-3.1.0:\n",
            "      Successfully uninstalled cloudpickle-3.1.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: tensorflow_hub\n",
            "    Found existing installation: tensorflow-hub 0.16.1\n",
            "    Uninstalling tensorflow-hub-0.16.1:\n",
            "      Successfully uninstalled tensorflow-hub-0.16.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.8.0\n",
            "    Uninstalling matplotlib-3.8.0:\n",
            "      Successfully uninstalled matplotlib-3.8.0\n",
            "  Attempting uninstall: jsonschema\n",
            "    Found existing installation: jsonschema 4.23.0\n",
            "    Uninstalling jsonschema-4.23.0:\n",
            "      Successfully uninstalled jsonschema-4.23.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2024.10.0\n",
            "    Uninstalling dask-2024.10.0:\n",
            "      Successfully uninstalled dask-2024.10.0\n",
            "  Attempting uninstall: CacheControl\n",
            "    Found existing installation: CacheControl 0.14.1\n",
            "    Uninstalling CacheControl-0.14.1:\n",
            "      Successfully uninstalled CacheControl-0.14.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.5.2\n",
            "    Uninstalling scikit-learn-1.5.2:\n",
            "      Successfully uninstalled scikit-learn-1.5.2\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.11.2\n",
            "    Uninstalling aiohttp-3.11.2:\n",
            "      Successfully uninstalled aiohttp-3.11.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.19 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.9 which is incompatible.\n",
            "bigframes 1.27.0 requires matplotlib>=3.7.1, but you have matplotlib 3.5.3 which is incompatible.\n",
            "bigframes 1.27.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.27.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "chex 0.1.87 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n",
            "langchain 0.3.7 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.10.9 which is incompatible.\n",
            "langchain-core 0.3.19 requires packaging<25,>=23.2, but you have packaging 20.9 which is incompatible.\n",
            "langchain-core 0.3.19 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.10.9 which is incompatible.\n",
            "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.1.3 which is incompatible.\n",
            "nx-cugraph-cu12 24.10.0 requires networkx>=3.0, but you have networkx 2.6.3 which is incompatible.\n",
            "pandas-gbq 0.24.0 requires packaging>=22.0.0, but you have packaging 20.9 which is incompatible.\n",
            "plotnine 0.14.1 requires matplotlib>=3.8.0, but you have matplotlib 3.5.3 which is incompatible.\n",
            "referencing 0.35.1 requires attrs>=22.2.0, but you have attrs 22.1.0 which is incompatible.\n",
            "scikit-image 0.24.0 requires networkx>=2.8, but you have networkx 2.6.3 which is incompatible.\n",
            "scikit-image 0.24.0 requires packaging>=21, but you have packaging 20.9 which is incompatible.\n",
            "shap 0.46.0 requires packaging>20.9, but you have packaging 20.9 which is incompatible.\n",
            "sphinx 8.1.3 requires packaging>=23.0, but you have packaging 20.9 which is incompatible.\n",
            "statsmodels 0.14.4 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.12.0 which is incompatible.\n",
            "wandb 0.18.7 requires sentry-sdk>=2.0.0, but you have sentry-sdk 1.14.0 which is incompatible.\n",
            "xarray 2024.10.0 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2024.10.0 requires packaging>=23.1, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed CacheControl-0.12.14 SQLAlchemy-1.4.54 aio-pika-8.2.3 aiofiles-24.1.0 aiogram-2.15 aiohttp-3.9.5 aiohttp-retry-2.9.1 aiormq-6.4.2 apscheduler-3.9.1.post1 attrs-22.1.0 bidict-0.23.1 boto3-1.35.69 botocore-1.35.69 cloudpickle-2.2.1 colorclass-2.2.2 coloredlogs-15.0.1 colorhash-1.2.1 confluent-kafka-2.6.1 dask-2022.10.2 dnspython-2.3.0 docopt-0.6.2 fbmessenger-6.0.0 fire-0.7.0 gast-0.4.0 google-auth-oauthlib-1.0.0 httptools-0.6.4 humanfriendly-10.0 jax-0.4.30 jaxlib-0.4.30 jmespath-1.0.1 joblib-1.2.0 jsonpickle-3.0.4 jsonschema-4.17.3 keras-2.12.0 matplotlib-3.5.3 mattermostwrapper-2.2 multidict-5.2.0 networkx-2.6.3 numpy-1.23.5 packaging-20.9 pamqp-3.2.1 portalocker-2.10.1 prompt-toolkit-3.0.28 protobuf-4.23.3 psycopg2-binary-2.9.10 pydantic-1.10.9 pydot-1.4.2 pykwalify-1.8.0 pymongo-4.3.3 pyrsistent-0.20.0 python-crfsuite-0.9.11 python-engineio-4.10.1 python-socketio-5.11.4 pytz-2022.7.1 questionary-1.10.0 randomname-0.1.5 rasa-3.6.20 rasa-sdk-3.6.2 redis-4.6.0 regex-2022.10.31 rocketchat_API-1.30.0 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.12 s3transfer-0.10.4 sanic-21.12.2 sanic-cors-2.0.1 sanic-jwt-1.8.0 sanic-routing-0.7.2 scikit-learn-1.1.3 scipy-1.10.1 sentry-sdk-1.14.0 simple-websocket-1.1.0 sklearn-crfsuite-0.3.6 slack-sdk-3.33.4 structlog-23.3.0 structlog-sentry-2.1.0 tarsafe-0.0.4 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 tensorflow-text-2.12.0 tensorflow_hub-0.13.0 terminaltables-3.1.10 twilio-8.2.2 typing-utils-0.1.0 ujson-5.10.0 uvloop-0.21.0 webexteamssdk-1.6.1 websockets-10.4 wrapt-1.14.1 wsproto-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "packaging",
                  "prompt_toolkit"
                ]
              },
              "id": "78e29f0ed2ce46adaa0e45523b16f96b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ipython\n",
        "# restart runtime: Runtime -> Restart runtime"
      ],
      "metadata": {
        "id": "iFypPLbJT6db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4_FQKauLS2FE"
      },
      "outputs": [],
      "source": [
        "# legacy setup, left for consistency\n",
        "# !pip install --upgrade pip==20.2\n",
        "# !pip install ipython\n",
        "# !pip install nest_asyncio\n",
        "# !pip install tensoflow <= 2.4\n",
        "# !pip install -U rasa\n",
        "# !pip install awscli --ignore-installed six\n",
        "\n",
        "# !pip install -U ipython\n",
        "# restart runtime: Runtime -> Restart runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTiOyCEYTD8Z"
      },
      "source": [
        "## **First steps with RASA chatbots**\n",
        "\n",
        "Before diving into the practice, let's see how to use the RASA framework to build a simple chatbot. Use the simple chatbot example provided in the RASA documentation to build the simplest chatbot possible. The following cells will guide you through the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZPM-_lwTK-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93d167fc-5843-4096-8dd2-54b8891418b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Event loop ready.\n",
            "config.yml data/ domain.yml models/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasa/engine/caching.py:152: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base: DeclarativeMeta = declarative_base()\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b(0lqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqk\u001b(B\n",
            "\u001b(0x\u001b(B Rasa Open Source reports anonymous usage telemetry to help improve the product \u001b(0x\u001b(B\n",
            "\u001b(0x\u001b(B for all its users.                                                             \u001b(0x\u001b(B\n",
            "\u001b(0x\u001b(B                                                                                \u001b(0x\u001b(B\n",
            "\u001b(0x\u001b(B If you'd like to opt-out, you can use `rasa telemetry disable`.                \u001b(0x\u001b(B\n",
            "\u001b(0x\u001b(B To learn more, check out https://rasa.com/docs/rasa/telemetry/telemetry.       \u001b(0x\u001b(B\n",
            "\u001b(0mqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqj\u001b(B\n",
            "\u001b[94mThe configuration for pipeline and policies was chosen automatically. It was written into the config file at 'config.yml'.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "Epochs:   0%|          | 0/100 [00:00<?, ?it/s]/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
            "  return self.randrange(a, b+1)\n",
            "Epochs: 100%|██████████| 100/100 [00:33<00:00,  3.01it/s, t_loss=1.1, i_acc=1]\n",
            "Processed story blocks: 100%|██████████| 3/3 [00:00<00:00, 380.55it/s, # trackers=1]\n",
            "Processed story blocks: 100%|██████████| 3/3 [00:00<00:00, 274.32it/s, # trackers=3]\n",
            "Processed story blocks: 100%|██████████| 3/3 [00:00<00:00, 187.46it/s, # trackers=12]\n",
            "Processed story blocks: 100%|██████████| 3/3 [00:00<00:00, 63.12it/s, # trackers=39]\n",
            "Processed rules: 100%|██████████| 2/2 [00:00<00:00, 475.71it/s, # trackers=1]\n",
            "Processed trackers: 100%|██████████| 3/3 [00:00<00:00, 165.41it/s, # action=12]\n",
            "Processed actions: 12it [00:00, 598.27it/s, # examples=12]\n",
            "Processed trackers: 100%|██████████| 2/2 [00:00<00:00, 168.22it/s, # action=5]\n",
            "Processed actions: 5it [00:00, 916.19it/s, # examples=4]\n",
            "Processed trackers: 100%|██████████| 3/3 [00:00<00:00, 196.23it/s, # action=12]\n",
            "Processed trackers:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: goodbye | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: goodbye | previous action name: action_listen\n",
            "[state 3] user intent: goodbye | previous action name: utter_goodbye\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 3] user intent: bot_challenge | previous action name: utter_iamabot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed trackers: 100%|██████████| 2/2 [00:00<00:00, 207.58it/s]\n",
            "Processed trackers:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_great | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_great | previous action name: action_listen\n",
            "[state 4] user intent: mood_great | previous action name: utter_happy\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] user intent: affirm | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] user intent: affirm | previous action name: action_listen\n",
            "[state 7] user intent: affirm | previous action name: utter_happy\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] user intent: deny | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "[state 4] user intent: mood_unhappy | previous action name: utter_cheer_up\n",
            "[state 5] user intent: mood_unhappy | previous action name: utter_did_that_help\n",
            "[state 6] user intent: deny | previous action name: action_listen\n",
            "[state 7] user intent: deny | previous action name: utter_goodbye\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: goodbye | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: goodbye | previous action name: action_listen\n",
            "[state 3] user intent: goodbye | previous action name: utter_goodbye\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 17:43:35 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 3] user intent: bot_challenge | previous action name: utter_iamabot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed trackers: 100%|██████████| 5/5 [00:00<00:00, 97.65it/s]\n",
            "Processed trackers: 100%|██████████| 120/120 [00:00<00:00, 1104.26it/s, # action=30]\n",
            "/usr/local/lib/python3.10/dist-packages/rasa/utils/tensorflow/model_data.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(ragged_array)\n",
            "Epochs: 100%|██████████| 100/100 [00:17<00:00,  5.78it/s, t_loss=2.77, loss=2.61, acc=0.967]\n",
            "WARNING:rasa.shared.utils.common:The UnexpecTED Intent Policy is currently experimental and might change or be removed in the future 🔬 Please share your feedback on it in the forum (https://forum.rasa.com) to help us make this feature ready for production.\n",
            "Processed trackers: 100%|██████████| 120/120 [00:00<00:00, 1853.67it/s, # intent=12]\n",
            "Epochs: 100%|██████████| 100/100 [00:15<00:00,  6.51it/s, t_loss=0.137, loss=0.0222, acc=1]\n",
            "/usr/local/lib/python3.10/dist-packages/rasa/core/policies/unexpected_intent_policy.py:839: DeprecationWarning: the `interpolation=` argument to quantile was renamed to `method=`, which has additional options.\n",
            "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
            "  quantile_values = np.quantile(  # type: ignore[call-overload]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mYour Rasa model is trained and saved at 'models/20241126-174259-humane-sink.tar.gz'.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "import rasa\n",
        "import nest_asyncio\n",
        "from rasa.cli.scaffold import create_initial_project\n",
        "\n",
        "os.chdir(\"/content/\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "print(\"Event loop ready.\")\n",
        "\n",
        "project = \"my-chatbot\"\n",
        "create_initial_project(project)\n",
        "os.chdir(project)\n",
        "\n",
        "config = \"config.yml\"\n",
        "training_files = \"data/\"\n",
        "domain = \"domain.yml\"\n",
        "output = \"models/\"\n",
        "print(config, training_files, domain, output)\n",
        "\n",
        "model_path = rasa.train(domain, config, [training_files], output)\n",
        "model_path = model_path.model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnUsn5jlTbNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b293cd29-d71c-4371-c7cc-996870e080b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
            "  return self.randrange(a, b+1)\n",
            "WARNING:rasa.shared.utils.common:The UnexpecTED Intent Policy is currently experimental and might change or be removed in the future 🔬 Please share your feedback on it in the forum (https://forum.rasa.com) to help us make this feature ready for production.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your bot is ready to talk! Type your messages here or send '/stop'.\n",
            "Hello\n",
            "2024-11-26 17:45:16 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:45:16 [debug    ] processor.actions.log          action_name=action_session_start rasa_events=[<rasa.shared.core.events.SessionStarted object at 0x7f6441ad4490>, ActionExecuted(action: action_listen, policy: None, confidence: None)]\n",
            "2024-11-26 17:45:16 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 17:45:17 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'greet', 'confidence': 0.9999998807907104} parse_data_text=Hello\n",
            "2024-11-26 17:45:17 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 17:45:17 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 17:45:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user text: Hello | previous action name: action_listen\n",
            "2024-11-26 17:45:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 17:45:17 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f6441ad49a0>]\n",
            "2024-11-26 17:45:17 [debug    ] processor.actions.log          action_name=utter_greet rasa_events=[BotUttered('Hey! How are you?', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_greet\"}, 1732643117.3486116)]\n",
            "2024-11-26 17:45:17 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}]\n",
            "2024-11-26 17:45:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 17:45:17 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:45:17 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "\u001b[92mHey! How are you?\u001b[0m\n",
            "good, and you?\n",
            "2024-11-26 17:45:25 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'bot_challenge', 'confidence': 0.6467926502227783} parse_data_text=good, and you?\n",
            "2024-11-26 17:45:25 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 17:45:25 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 17:45:25 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user text: good, and you? | previous action name: action_listen\n",
            "2024-11-26 17:45:25 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 17:45:25 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f6441ad5090>]\n",
            "2024-11-26 17:45:25 [debug    ] processor.actions.log          action_name=utter_iamabot rasa_events=[BotUttered('I am a bot, powered by Rasa.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_iamabot\"}, 1732643125.09077)]\n",
            "2024-11-26 17:45:25 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 17:45:25 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 4] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "2024-11-26 17:45:25 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:45:25 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "\u001b[92mI am a bot, powered by Rasa.\u001b[0m\n",
            "/stop\n"
          ]
        }
      ],
      "source": [
        "from rasa.jupyter import chat\n",
        "\n",
        "endpoints = None\n",
        "chat(model_path, endpoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Tys9K2TKfS"
      },
      "source": [
        "## **Your own chatbot**\n",
        "\n",
        "RASA chatbots exploit the following files to recognize intents and take the corresponding actions:\n",
        "\n",
        "- `/data/nlu.yml`: it contains the set of intents that are used by the chatbot for recognizing user requests. This file contains examples that are used to generate examples to train the chatbot. See more: NLU in [RASA doc](https://rasa.com/docs/rasa/training-data-format/#nlu-training-data).\n",
        "\n",
        "- `stories.yml`: this file contains the examples of interactions between the chatbot and the user. They define possible paths of the conversations with corresponding chatbot actions and/or responses for each user input. See more: Stories in [RASA doc](https://rasa.com/docs/rasa/stories).\n",
        "\n",
        "- `domain.yml`: according to the official documentation: `The domain defines the universe in which your assistant operates. It specifies the intents, entities, slots, responses, forms, and actions your bot should know about.` This file contains a list of information that your chatbot needs to know to operate. See more: Domain in [RASA doc](https://rasa.com/docs/rasa/domain/).\n",
        "\n",
        "\n",
        "Modify the base chatbot to recognize one or multiple new intents (e.g., user looking for the weather)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPJSNzx1W-2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "582dc5ae-2b7e-459b-cfbd-1d05ac6ab27f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[94mThe configuration for pipeline and policies was chosen automatically. It was written into the config file at 'config.yml'.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epochs: 100%|██████████| 100/100 [00:25<00:00,  3.98it/s, t_loss=1.12, i_acc=1]\n",
            "Processed story blocks: 100%|██████████| 4/4 [00:00<00:00, 425.05it/s, # trackers=1]\n",
            "Processed story blocks: 100%|██████████| 4/4 [00:00<00:00, 430.48it/s, # trackers=4]\n",
            "Processed story blocks: 100%|██████████| 4/4 [00:00<00:00, 95.13it/s, # trackers=20]\n",
            "Processed story blocks: 100%|██████████| 4/4 [00:00<00:00, 67.59it/s, # trackers=50]\n",
            "Processed rules: 100%|██████████| 1/1 [00:00<00:00, 523.96it/s, # trackers=1]\n",
            "Processed trackers: 100%|██████████| 4/4 [00:00<00:00, 215.72it/s, # action=9]\n",
            "Processed actions: 9it [00:00, 939.02it/s, # examples=9]\n",
            "Processed trackers: 100%|██████████| 1/1 [00:00<00:00, 150.59it/s, # action=3]\n",
            "Processed actions: 3it [00:00, 1044.23it/s, # examples=2]\n",
            "Processed trackers: 100%|██████████| 4/4 [00:00<00:00, 422.32it/s, # action=9]\n",
            "Processed trackers:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: nlu_fallback | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: nlu_fallback | previous action name: action_listen\n",
            "[state 3] user intent: nlu_fallback | previous action name: utter_iamabot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed trackers: 100%|██████████| 1/1 [00:00<00:00, 254.11it/s]\n",
            "Processed trackers:   0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: goodbye | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: goodbye | previous action name: action_listen\n",
            "[state 2] user intent: goodbye | previous action name: utter_goodbye\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 2] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "[state 2] user intent: get_weather | previous action name: utter_weather\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: nlu_fallback | previous action name: action_listen\n",
            "2024-11-26 17:49:42 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] previous action name: ...\n",
            "[state 2] user intent: nlu_fallback | previous action name: action_listen\n",
            "[state 3] user intent: nlu_fallback | previous action name: utter_iamabot\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed trackers: 100%|██████████| 5/5 [00:00<00:00, 288.26it/s]\n",
            "Processed trackers: 100%|██████████| 244/244 [00:00<00:00, 539.77it/s, # action=169]\n",
            "/usr/local/lib/python3.10/dist-packages/rasa/utils/tensorflow/model_data.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  return np.array(ragged_array)\n",
            "Epochs: 100%|██████████| 100/100 [00:24<00:00,  4.09it/s, t_loss=0.237, loss=0.073, acc=1]\n",
            "WARNING:rasa.shared.utils.common:The UnexpecTED Intent Policy is currently experimental and might change or be removed in the future 🔬 Please share your feedback on it in the forum (https://forum.rasa.com) to help us make this feature ready for production.\n",
            "Processed trackers: 100%|██████████| 244/244 [00:00<00:00, 648.02it/s, # intent=61]\n",
            "Epochs: 100%|██████████| 100/100 [00:17<00:00,  5.64it/s, t_loss=0.111, loss=0.00578, acc=1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[92mYour Rasa model is trained and saved at 'models/20241126-174910-partial-luggage.tar.gz'.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
            "  return self.randrange(a, b+1)\n",
            "WARNING:rasa.shared.utils.common:The UnexpecTED Intent Policy is currently experimental and might change or be removed in the future 🔬 Please share your feedback on it in the forum (https://forum.rasa.com) to help us make this feature ready for production.\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <bound method RasaModel.predict_step of <rasa.core.policies.ted_policy.TED object at 0x7f6437c79a20>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your bot is ready to talk! Type your messages here or send '/stop'.\n",
            "how is weather?\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.log          action_name=action_session_start rasa_events=[<rasa.shared.core.events.SessionStarted object at 0x7f6443537640>, ActionExecuted(action: action_listen, policy: None, confidence: None)]\n",
            "2024-11-26 17:51:18 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 17:51:18 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'get_weather', 'confidence': 0.9999924898147583} parse_data_text=how is weather?\n",
            "2024-11-26 17:51:18 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 17:51:18 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 17:51:18 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user text: how is weather? | previous action name: action_listen\n",
            "2024-11-26 17:51:18 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643aa47ac0>]\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.log          action_name=utter_weather rasa_events=[BotUttered('The weather today is sunny with a high of 25 degrees Celsius.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_weather\"}, 1732643478.8973396)]\n",
            "2024-11-26 17:51:18 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}]\n",
            "2024-11-26 17:51:18 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "[state 2] user intent: get_weather | previous action name: utter_weather\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:51:18 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "\u001b[92mThe weather today is sunny with a high of 25 degrees Celsius.\u001b[0m\n",
            "do you have feeling?\n",
            "2024-11-26 17:52:22 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'mood_great', 'confidence': 0.9921253323554993} parse_data_text=do you have feeling?\n",
            "2024-11-26 17:52:22 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 17:52:22 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'mood_great'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 17:52:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "[state 2] user intent: get_weather | previous action name: utter_weather\n",
            "[state 3] user text: do you have feeling? | previous action name: action_listen\n",
            "2024-11-26 17:52:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "[state 2] user intent: get_weather | previous action name: utter_weather\n",
            "[state 3] user intent: mood_great | previous action name: action_listen\n",
            "2024-11-26 17:52:22 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643aa44fa0>]\n",
            "2024-11-26 17:52:22 [debug    ] processor.actions.log          action_name=utter_goodbye rasa_events=[BotUttered('Bye! Take care.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_goodbye\"}, 1732643542.3650022)]\n",
            "2024-11-26 17:52:22 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'mood_great'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'mood_great'}, 'prev_action': {'action_name': 'utter_goodbye'}}]\n",
            "2024-11-26 17:52:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: get_weather | previous action name: action_listen\n",
            "[state 2] user intent: get_weather | previous action name: utter_weather\n",
            "[state 3] user intent: mood_great | previous action name: action_listen\n",
            "[state 4] user intent: mood_great | previous action name: utter_goodbye\n",
            "2024-11-26 17:52:22 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 17:52:22 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "\u001b[92mBye! Take care.\u001b[0m\n",
            "/stop\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import rasa\n",
        "import nest_asyncio\n",
        "from rasa.cli.scaffold import create_initial_project\n",
        "\n",
        "\n",
        "project = \"my-chatbot\"\n",
        "if not os.path.exists(project):\n",
        "    create_initial_project(project)\n",
        "os.chdir(project)\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "config = \"config.yml\"\n",
        "training_files = \"data/\"\n",
        "domain = \"domain.yml\"\n",
        "output = \"models/\"\n",
        "\n",
        "\n",
        "nlu_data = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "nlu:\n",
        "- intent: greet\n",
        "  examples: |\n",
        "    - hey\n",
        "    - hello\n",
        "    - hi\n",
        "    - good morning\n",
        "    - good evening\n",
        "    - hey there\n",
        "\n",
        "- intent: goodbye\n",
        "  examples: |\n",
        "    - bye\n",
        "    - goodbye\n",
        "    - see you around\n",
        "    - see you later\n",
        "\n",
        "- intent: affirm\n",
        "  examples: |\n",
        "    - yes\n",
        "    - indeed\n",
        "    - of course\n",
        "    - that sounds good\n",
        "    - correct\n",
        "\n",
        "- intent: deny\n",
        "  examples: |\n",
        "    - no\n",
        "    - never\n",
        "    - I don't think so\n",
        "    - don't like that\n",
        "    - no way\n",
        "    - not really\n",
        "\n",
        "- intent: mood_great\n",
        "  examples: |\n",
        "    - perfect\n",
        "    - great\n",
        "    - amazing\n",
        "    - feeling like a king\n",
        "    - wonderful\n",
        "    - I am feeling very good\n",
        "    - I am great\n",
        "    - I'm good\n",
        "\n",
        "- intent: mood_unhappy\n",
        "  examples: |\n",
        "    - sad\n",
        "    - very sad\n",
        "    - unhappy\n",
        "    - bad\n",
        "    - I don't feel very well\n",
        "    - I am sad\n",
        "    - I am depressed\n",
        "    - I'm not feeling very well\n",
        "\n",
        "- intent: bot_challenge\n",
        "  examples: |\n",
        "    - are you a bot?\n",
        "    - are you a human?\n",
        "    - am I talking to a bot?\n",
        "    - am I talking to a human?\n",
        "\n",
        "- intent: get_weather\n",
        "  examples: |\n",
        "    - What's the weather like today?\n",
        "    - Tell me the weather forecast.\n",
        "    - How's the weather?\n",
        "    - Is it going to rain today?\n",
        "    - Do I need an umbrella today?\n",
        "    - What's the temperature outside?\n",
        "\"\"\"\n",
        "\n",
        "with open('data/nlu.yml', 'w') as f:\n",
        "    f.write(nlu_data)\n",
        "\n",
        "\n",
        "domain_data = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "intents:\n",
        "  - greet\n",
        "  - goodbye\n",
        "  - affirm\n",
        "  - deny\n",
        "  - mood_great\n",
        "  - mood_unhappy\n",
        "  - bot_challenge\n",
        "  - get_weather\n",
        "\n",
        "responses:\n",
        "  utter_greet:\n",
        "    - text: \"Hey! How are you?\"\n",
        "\n",
        "  utter_cheer_up:\n",
        "    - text: \"Here is something to cheer you up!\"\n",
        "      image: \"https://i.imgur.com/nGF1K8f.jpg\"\n",
        "\n",
        "  utter_did_that_help:\n",
        "    - text: \"Did that help you?\"\n",
        "\n",
        "  utter_goodbye:\n",
        "    - text: \"Bye! Take care.\"\n",
        "\n",
        "  utter_happy:\n",
        "    - text: \"Great, carry on!\"\n",
        "\n",
        "  utter_bot_challenge:\n",
        "    - text: \"I am a bot, powered by Rasa.\"\n",
        "\n",
        "  utter_weather:\n",
        "    - text: \"The weather today is sunny with a high of 25 degrees Celsius.\"\n",
        "\n",
        "  utter_iamabot:\n",
        "    - text: \"I am a bot, powered by Rasa.\"\n",
        "\n",
        "session_config:\n",
        "  session_expiration_time: 60\n",
        "  carry_over_slots_to_new_session: true\n",
        "\"\"\"\n",
        "\n",
        "with open('domain.yml', 'w') as f:\n",
        "    f.write(domain_data)\n",
        "\n",
        "\n",
        "stories_data = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "stories:\n",
        "- story: greet user\n",
        "  steps:\n",
        "  - intent: greet\n",
        "  - action: utter_greet\n",
        "\n",
        "- story: say goodbye\n",
        "  steps:\n",
        "  - intent: goodbye\n",
        "  - action: utter_goodbye\n",
        "\n",
        "- story: get weather\n",
        "  steps:\n",
        "  - intent: get_weather\n",
        "  - action: utter_weather\n",
        "\n",
        "- story: bot challenge\n",
        "  steps:\n",
        "  - intent: bot_challenge\n",
        "  - action: utter_iamabot\n",
        "\"\"\"\n",
        "\n",
        "with open('data/stories.yml', 'w') as f:\n",
        "    f.write(stories_data)\n",
        "\n",
        "\n",
        "rules_data = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "rules:\n",
        "- rule: Fallback rule\n",
        "  steps:\n",
        "  - intent: nlu_fallback\n",
        "  - action: utter_iamabot\n",
        "\"\"\"\n",
        "\n",
        "with open('data/rules.yml', 'w') as f:\n",
        "    f.write(rules_data)\n",
        "\n",
        "\n",
        "model_path = rasa.train(domain, config, [training_files], output)\n",
        "model_path = model_path.model\n",
        "\n",
        "\n",
        "from rasa.jupyter import chat\n",
        "\n",
        "endpoints = None\n",
        "chat(model_path, endpoints)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEEoqRyXbIaB"
      },
      "source": [
        "# Create a chatbot using Transformers\n",
        "\n",
        "RASA is a powerful framework that allows to build chatbots with a wide range of functionalities. It can rely on external models for the the generation of new responses. In this section, we will use the Transformers library to create a chatbot that can generate responses to user requests.\n",
        "\n",
        "![](https://huggingface.co/front/thumbnails/dialogpt.png)\n",
        "\n",
        "On the other side, [HuggingFace pipeline module](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines) offers an easy interface to use pre-trained models. In particular, we will use the `pipeline` function to create a chatbot that can generate responses to user requests (e.g., DialoGPT). The conversational pipeline allows the implementation of a simple chatbot with carry-on conversations. It exploits the DialoGPT models available on the model hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpFpFIFHb6mN"
      },
      "source": [
        "##DialoGPT single answer**\n",
        "\n",
        "Creating a conversational pipeline. The code that takes a user request and returns the generated response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro3KIC2JW4Dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573,
          "referenced_widgets": [
            "f06ba9a062a343bb8373b95cf53d3452",
            "0d1d183e5afe413fbddc4308bf758c98",
            "ae233e1fb2cf45b78345fb1ce82624a0",
            "50a70c7034034b2ca2cf3a006fd9fc25",
            "42c15e5e07484887a813afd0f2f9a4d7",
            "9614cea6d0284a03aa7bc813434c99d8",
            "564186891028403283c7f29501883831",
            "0acd84f2d6e04d5bb6d0f290d2420c4d",
            "0f86f80fe2414609b763bcccd2c5dc3e",
            "5f290c903376480cb3561a66fd597aff",
            "116e256d832c46e796122fa090b6f0f4",
            "e59ecb652d9e4deba9b51a7229c67bd7",
            "0fa552ad45d442f1b1a758e217116a08",
            "5f09b35065ba4e0093f3dac3c40de56b",
            "35aa9950e189457487192d226442ff34",
            "af45cee6512d485d8fe5b830783f107d",
            "e620c21e0c804021b5c8f3ebbf6ef9c1",
            "dd51b5dd43ab478eb065c7ce653dbfb6",
            "3d71e83251bc45ff95fd7d9d254804a9",
            "2c6c09c9d302493c83d3f96f6c8fe281",
            "ab3f7696e1824371a67b2fa72b15d537",
            "730c388213674bc5b7f03dd4848f6d39",
            "2470044f68ca4abebf57b06210457a2c",
            "8a12548a35ae498c804fe90e9878b4ec",
            "87500b89e1a34eba955dbfe1486e1766",
            "7bfc411e63fd4c19b84c347731bae0ed",
            "db640df184e34ca0b3fc5b92c531fe80",
            "dd366d7a5b3a41738e14c6ecd7931fd8",
            "c68c2fcd5f18493698f93de87765cf0e",
            "90329dd8ff07417a996ec3c89f667432",
            "a804c677de384bd0a364d2e5fc0bad92",
            "e8c95ed550574e9da7c94e900e39c184",
            "0628c31bc2fe4075bd445b0d842eacc8",
            "bf3de6caed00477582782ad832c1a6f1",
            "f69eb1a1ce71470ab03dfecc2f1d35d3",
            "e07b5cbe48fb4b5d85c9f4113544172b",
            "56acb681a2e046cb81bfc433425d9fa2",
            "330fdcb967144027942d78848400d0eb",
            "004553eb3c9d45d28029c0655240f618",
            "07829a4504fe4a5c84771228de4a0c13",
            "490d5c4cd7a84643848568b29f80920d",
            "b38e54ca60d94b2ea040f315f6f63154",
            "450910e7d66540fab8cbbbd1c6d82d1c",
            "1ef5f03bf1dc4258832da43f79838376",
            "9abb166e28e742eb95ed5ba12947d751",
            "5da2541399ce4b429eac088a0eb9e749",
            "b884a897dbf044c396680a153c28708d",
            "0f4a740efdd7476fa3622cccbb26e5b1",
            "b8911881fb9e465190112ef3fac38db0",
            "8035771c30df4f1bb90cd5feea219feb",
            "2780128fc92a4e5484f26784401f01d2",
            "bf6a9690f6d743d8b3c0ef1e9c2bc078",
            "452eb7ec76a3490686521cf2ae4089aa",
            "ce4fbf40196b4c14ac8f406efd18838f",
            "e356fa19988b43be8de48db99f3a1053",
            "aa6efd57fff040eeb8e918389af4199e",
            "882a9a37105e4ff4b32493b6856a12af",
            "568176484cda471da3f4978001e0e6af",
            "6a894bbbb63748d08abc8803ba832cf1",
            "e7891f898e164d55adecaa2e8de39d1a",
            "8bd5f11de2c344e2961689e0e4503378",
            "4c5bc227971b426bb61f2219cace91d6",
            "c540be215d3f4bb28280d2f2ebe1ab81",
            "f08d01fc5fae480d99821692461cd78f",
            "ed61fd8f52b94fd38a8a15104335d697",
            "d0ad408b7f834477a1cd6139dfe74d9f",
            "4bb619be2eff4c55ba9c864ceff5ed33",
            "35991b2da4cd45559243c6e8b1405ce6",
            "422588c2cc91476eb186459196503b3e",
            "dd357dd22dc54d788578cf34f645368a",
            "5d71f786a23843eea32a54aa8f6d88a5",
            "d67c6616e9a94bbcacfb55a31ce61280",
            "6672ac26d0d646399c9df69a87a26ebb",
            "aad1307ed3a545d9a63e5145f362b6b8",
            "4b37c3e7705043a4b93b70ed5ad34511",
            "1f19d9903f934a05829fedd69aa7472c",
            "133c431a9378467fa27baf47dd283291"
          ]
        },
        "outputId": "394891c3-f36f-4db0-b71a-f4589a90f25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f06ba9a062a343bb8373b95cf53d3452"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e59ecb652d9e4deba9b51a7229c67bd7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2470044f68ca4abebf57b06210457a2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/642 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf3de6caed00477582782ad832c1a6f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9abb166e28e742eb95ed5ba12947d751"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.75G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa6efd57fff040eeb8e918389af4199e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4bb619be2eff4c55ba9c864ceff5ed33"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialoGPT Chatbot\n",
            "Type 'exit' to end the conversation.\n",
            "\n",
            "You: how is weather?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot: Not too hot and not too cold\n",
            "\n",
            "You: do you have any feeling?\n",
            "Chatbot: I feel like a very warm blanket.\n",
            "\n",
            "You: exit\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "\n",
        "def generate_response(user_input, chat_history_ids=None):\n",
        "    \"\"\"\n",
        "    Generates a response to the user input using DialoGPT.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "        chat_history_ids (torch.Tensor, optional): The history of the conversation.\n",
        "\n",
        "    Returns:\n",
        "        response (str): The generated response from the model.\n",
        "        chat_history_ids (torch.Tensor): Updated chat history including the latest user input and model response.\n",
        "    \"\"\"\n",
        "\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.75\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids\n",
        "\n",
        "def chat():\n",
        "    print(\"DialoGPT Chatbot\\nType 'exit' to end the conversation.\\n\")\n",
        "    chat_history_ids = None\n",
        "\n",
        "    while True:\n",
        "\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        response, chat_history_ids = generate_response(user_input, chat_history_ids)\n",
        "\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ytWSQJq7dfr"
      },
      "source": [
        "## **Question 4: Conversations**\n",
        "\n",
        "Extend the previous function to generate a conversation with DialoGPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2BnAufo7c5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3344320-f3c2-4154-94f8-d4dd23c4f5cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DialoGPT Chatbot\n",
            "Type 'exit', 'bye', or 'goodbye' to end the conversation.\n",
            "\n",
            "You: hello. how are you?\n",
            "Chatbot: I'm good\n",
            "\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
        "import torch\n",
        "\n",
        "class StopSequenceCriteria(StoppingCriteria):\n",
        "    def __init__(self, stop_sequence_ids):\n",
        "        self.stop_sequence_ids = stop_sequence_ids\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "\n",
        "        if input_ids.shape[-1] < len(self.stop_sequence_ids):\n",
        "            return False\n",
        "\n",
        "\n",
        "        if torch.all(\n",
        "            input_ids[0, -len(self.stop_sequence_ids):] == torch.tensor(self.stop_sequence_ids)\n",
        "        ):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "# Define the stop sequence\n",
        "STOP_SEQUENCE = \"Goodbye!\"\n",
        "stop_sequence_ids = tokenizer.encode(STOP_SEQUENCE, add_special_tokens=False)\n",
        "\n",
        "def generate_response(user_input, chat_history_ids=None):\n",
        "    \"\"\"\n",
        "    Generates a response to the user input using DialoGPT.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "        chat_history_ids (torch.Tensor, optional): The history of the conversation.\n",
        "\n",
        "    Returns:\n",
        "        response (str): The generated response from the model.\n",
        "        chat_history_ids (torch.Tensor): Updated chat history including the latest user input and model response.\n",
        "    \"\"\"\n",
        "    # encode the user input and add end-of-string token\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history (if exists)\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "\n",
        "    stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop_sequence_ids)])\n",
        "\n",
        "\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=3,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.75,\n",
        "        stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids\n",
        "\n",
        "def chat():\n",
        "    print(\"DialoGPT Chatbot\\nType 'exit', 'bye', or 'goodbye' to end the conversation.\\n\")\n",
        "    chat_history_ids = None\n",
        "\n",
        "\n",
        "    exit_commands = ['exit', 'bye', 'goodbye']\n",
        "\n",
        "    while True:\n",
        "\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "\n",
        "        if user_input.lower() in exit_commands:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        response, chat_history_ids = generate_response(user_input, chat_history_ids)\n",
        "\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "\n",
        "        if STOP_SEQUENCE.lower() in response.lower():\n",
        "            print(\"Chatbot: Conversation has ended as per the stop sequence.\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNapmc47r-J"
      },
      "source": [
        "## **Improving interaction**\n",
        "\n",
        "The conversations in the previous questions show very limited variability. Create a new function to manually conversate with DialoGPT model by setting different parameters (e.g., [beam search](https://en.wikipedia.org/wiki/Beam_search) is disabled by default). To do so, instantiate a new [DialoGPT model](https://huggingface.co/microsoft/DialoGPT-medium), which inherits from the `AutoModelForCausalLM`.\n",
        "\n",
        "Here it is a simple blog post that shows how to use different decoding strategies with DialoGPT: [https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate).\n",
        "\n",
        "**Note 1**: Basic examples on how to use the model are provided [here](https://huggingface.co/microsoft/DialoGPT-medium#how-to-use).\n",
        "\n",
        "**Note 2**: Take some time to explore the input for the [generate](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) function. Hereinafter some examples of relevant parameters.\n",
        "```\n",
        "num_beams (int, optional, defaults to 1) — Number of beams for beam search. 1 means no beam search.\n",
        "temperature (float, optional, defaults to 1.0) — The value used to module the next token probabilities.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmPmZdnrXLUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd8a38b-983e-4c7d-db9f-2e3eb6c91b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DialoGPT Enhanced Chatbot\n",
            "Type 'exit', 'bye', or 'goodbye' to end the conversation.\n",
            "\n",
            "You: hello\n",
            "\n",
            "[Optional] Set generation parameters or press Enter to use defaults.\n",
            "Enable sampling? (True/False) [Default: True]: True\n",
            "Number of beams for beam search [Default: 1]: 2\n",
            "Top-K tokens [Default: 50]: 40\n",
            "Top-P (nucleus sampling) [Default: 0.95]: \n",
            "Temperature [Default: 0.7]: \n",
            "No repeat n-gram size [Default: 3]: \n",
            "Enter a stop sequence (or press Enter to skip): \n",
            "Chatbot: Hiya! How are you?\n",
            "\n",
            "You: good\n",
            "\n",
            "[Optional] Set generation parameters or press Enter to use defaults.\n",
            "Enable sampling? (True/False) [Default: True]: \n",
            "Number of beams for beam search [Default: 1]: \n",
            "Top-K tokens [Default: 50]: \n",
            "Top-P (nucleus sampling) [Default: 0.95]: \n",
            "Temperature [Default: 0.7]: \n",
            "No repeat n-gram size [Default: 3]: \n",
            "Enter a stop sequence (or press Enter to skip): \n",
            "Chatbot: Good to hear, what is your favorite color?\n",
            "\n",
            "You: bye\n",
            "Chatbot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
        "import torch\n",
        "\n",
        "class StopSequenceCriteria(StoppingCriteria):\n",
        "    def __init__(self, stop_sequence_ids):\n",
        "        \"\"\"\n",
        "        Initializes the stopping criteria with the token IDs of the stop sequence.\n",
        "\n",
        "        Args:\n",
        "            stop_sequence_ids (list): List of token IDs representing the stop sequence.\n",
        "        \"\"\"\n",
        "        self.stop_sequence_ids = stop_sequence_ids\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        \"\"\"\n",
        "        Determines whether to stop generation based on the stop sequence.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): The sequence of generated token IDs.\n",
        "            scores (torch.Tensor): The scores for the next token predictions.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the stop sequence is detected; False otherwise.\n",
        "        \"\"\"\n",
        "\n",
        "        if input_ids.shape[-1] < len(self.stop_sequence_ids):\n",
        "            return False\n",
        "\n",
        "\n",
        "        last_tokens = input_ids[0, -len(self.stop_sequence_ids):].tolist()\n",
        "\n",
        "        return last_tokens == self.stop_sequence_ids\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "\n",
        "def generate_response(\n",
        "    user_input,\n",
        "    chat_history_ids=None,\n",
        "    do_sample=True,\n",
        "    num_beams=1,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_length=1000,\n",
        "    stop_sequence=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a response to the user input using DialoGPT with customizable parameters.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "        chat_history_ids (torch.Tensor, optional): The history of the conversation.\n",
        "        do_sample (bool): Whether to use sampling; otherwise, greedy decoding.\n",
        "        num_beams (int): Number of beams for beam search.\n",
        "        top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        top_p (float): The cumulative probability for nucleus sampling.\n",
        "        temperature (float): Temperature value for scaling logits.\n",
        "        no_repeat_ngram_size (int): Prevent repeating n-grams of this size.\n",
        "        max_length (int): The maximum length of the generated response.\n",
        "        stop_sequence (str, optional): A phrase that, if generated, will stop further generation.\n",
        "\n",
        "    Returns:\n",
        "        response (str): The generated response from the model.\n",
        "        chat_history_ids (torch.Tensor): Updated chat history including the latest user input and model response.\n",
        "    \"\"\"\n",
        "\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "\n",
        "    stopping_criteria = None\n",
        "    if stop_sequence:\n",
        "        stop_sequence_ids = tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
        "        stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop_sequence_ids)])\n",
        "\n",
        "\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=do_sample,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        num_beams=num_beams,\n",
        "        stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids\n",
        "\n",
        "def interactive_chatbot():\n",
        "    \"\"\"\n",
        "    Runs an interactive chat session with the DialoGPT chatbot, allowing parameter customization.\n",
        "    \"\"\"\n",
        "    print(\"DialoGPT Enhanced Chatbot\\nType 'exit', 'bye', or 'goodbye' to end the conversation.\\n\")\n",
        "    chat_history_ids = None\n",
        "\n",
        "\n",
        "    exit_commands = ['exit', 'bye', 'goodbye']\n",
        "\n",
        "    while True:\n",
        "\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "\n",
        "        if user_input.lower() in exit_commands:\n",
        "            print(\"Chatbot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        print(\"\\n[Optional] Set generation parameters or press Enter to use defaults.\")\n",
        "        try:\n",
        "            do_sample_input = input(\"Enable sampling? (True/False) [Default: True]: \").strip()\n",
        "            if do_sample_input.lower() == 'true':\n",
        "                do_sample = True\n",
        "            elif do_sample_input.lower() == 'false':\n",
        "                do_sample = False\n",
        "            else:\n",
        "                do_sample = True  # Default\n",
        "\n",
        "            num_beams_input = input(\"Number of beams for beam search [Default: 1]: \").strip()\n",
        "            num_beams = int(num_beams_input) if num_beams_input.isdigit() else 1\n",
        "\n",
        "            top_k_input = input(\"Top-K tokens [Default: 50]: \").strip()\n",
        "            top_k = int(top_k_input) if top_k_input.isdigit() else 50\n",
        "\n",
        "            top_p_input = input(\"Top-P (nucleus sampling) [Default: 0.95]: \").strip()\n",
        "            top_p = float(top_p_input) if top_p_input.replace('.', '', 1).isdigit() else 0.95\n",
        "\n",
        "            temperature_input = input(\"Temperature [Default: 0.7]: \").strip()\n",
        "            temperature = float(temperature_input) if temperature_input.replace('.', '', 1).isdigit() else 0.7\n",
        "\n",
        "            no_repeat_ngram_size_input = input(\"No repeat n-gram size [Default: 3]: \").strip()\n",
        "            no_repeat_ngram_size = int(no_repeat_ngram_size_input) if no_repeat_ngram_size_input.isdigit() else 3\n",
        "\n",
        "            stop_sequence = input(\"Enter a stop sequence (or press Enter to skip): \").strip()\n",
        "            stop_sequence = stop_sequence if stop_sequence else None\n",
        "\n",
        "        except ValueError:\n",
        "            print(\"Invalid input detected. Using default parameters.\\n\")\n",
        "            do_sample = True\n",
        "            num_beams = 1\n",
        "            top_k = 50\n",
        "            top_p = 0.95\n",
        "            temperature = 0.7\n",
        "            no_repeat_ngram_size = 3\n",
        "            stop_sequence = None\n",
        "\n",
        "\n",
        "        response, chat_history_ids = generate_response(\n",
        "            user_input,\n",
        "            chat_history_ids=chat_history_ids,\n",
        "            do_sample=do_sample,\n",
        "            num_beams=num_beams,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temperature=temperature,\n",
        "            no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "            stop_sequence=stop_sequence\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "\n",
        "        if stop_sequence and stop_sequence.lower() in response.lower():\n",
        "            print(\"Chatbot: Conversation has ended as per the stop sequence.\")\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    interactive_chatbot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvfkg1rn96Ay"
      },
      "source": [
        "## **Artificial conversations**\n",
        "\n",
        "Let the two previous chatbots interact with each other using preferred parameter configuration(s)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9iIvc77XNFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19dcafe-45ac-4131-dc73-dfb5a1d85820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasa/core/tracker_store.py:1044: MovedIn20Warning: \u001b[31mDeprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. \u001b[32mTo prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". \u001b[36mSet environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message.\u001b[0m (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base: DeclarativeMeta = declarative_base()\n",
            "usage: rasa [-h] [--version]\n",
            "            {init,run,shell,train,interactive,telemetry,test,visualize,data,export,x,evaluate} ...\n",
            "rasa: error: unrecognized arguments: --project rasa_project\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "if os.path.exists(\"rasa_project\"):\n",
        "    shutil.rmtree(\"rasa_project\")\n",
        "\n",
        "# Initialize RASA project\n",
        "!rasa init --no-prompt --project rasa_project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "print(\"Current Directory Contents:\")\n",
        "print(os.listdir('.'))\n",
        "\n",
        "\n",
        "if 'rasa_project' in os.listdir('.'):\n",
        "    print(\"\\n'rasa_project' directory exists.\")\n",
        "\n",
        "    print(\"Contents of 'rasa_project':\")\n",
        "    print(os.listdir('rasa_project'))\n",
        "\n",
        "\n",
        "    if 'data' in os.listdir('rasa_project'):\n",
        "        print(\"\\n'data' directory exists within 'rasa_project'.\")\n",
        "    else:\n",
        "        print(\"\\n'data' directory is missing within 'rasa_project'. Creating it now...\")\n",
        "        os.makedirs('rasa_project/data', exist_ok=True)\n",
        "        print(\"'rasa_project/data' directory created.\")\n",
        "else:\n",
        "    print(\"\\n'rasa_project' directory is missing. Initializing RASA project now...\")\n",
        "\n",
        "    !rasa init --no-prompt --project rasa_project\n",
        "\n",
        "\n",
        "    if 'rasa_project' in os.listdir('.'):\n",
        "        print(\"'rasa_project' directory successfully created.\")\n",
        "        if 'data' in os.listdir('rasa_project'):\n",
        "            print(\"'data' directory exists within 'rasa_project'.\")\n",
        "        else:\n",
        "            print(\"'data' directory is missing within 'rasa_project'. Creating it now...\")\n",
        "            os.makedirs('rasa_project/data', exist_ok=True)\n",
        "            print(\"'rasa_project/data' directory created.\")\n",
        "    else:\n",
        "        print(\"Failed to create 'rasa_project' directory. Please check for errors in the RASA initialization step.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69fzFc2zpfyz",
        "outputId": "87a4552d-fb59-4211-eeb4-70cea646ff06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Directory Contents:\n",
            "['config.yml', 'domain.yml', 'data']\n",
            "\n",
            "'rasa_project' directory is missing. Initializing RASA project now...\n",
            "/usr/local/lib/python3.10/dist-packages/rasa/core/tracker_store.py:1044: MovedIn20Warning: \u001b[31mDeprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. \u001b[32mTo prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". \u001b[36mSet environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message.\u001b[0m (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base: DeclarativeMeta = declarative_base()\n",
            "usage: rasa [-h] [--version]\n",
            "            {init,run,shell,train,interactive,telemetry,test,visualize,data,export,x,evaluate} ...\n",
            "rasa: error: unrecognized arguments: --project rasa_project\n",
            "Failed to create 'rasa_project' directory. Please check for errors in the RASA initialization step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if os.path.exists(\"rasa_project\"):\n",
        "    shutil.rmtree(\"rasa_project\")\n",
        "    print(\"'rasa_project' directory removed.\")\n",
        "\n",
        "\n",
        "!rasa init --no-prompt --project rasa_project\n",
        "\n",
        "if 'rasa_project' in os.listdir('.'):\n",
        "    print(\"'rasa_project' directory successfully created.\")\n",
        "    if 'data' in os.listdir('rasa_project'):\n",
        "        print(\"'data' directory exists within 'rasa_project'.\")\n",
        "    else:\n",
        "        print(\"'data' directory is missing within 'rasa_project'. Creating it now...\")\n",
        "        os.makedirs('rasa_project/data', exist_ok=True)\n",
        "        print(\"'rasa_project/data' directory created.\")\n",
        "else:\n",
        "    print(\"Failed to create 'rasa_project' directory. Please check for errors in the RASA initialization step.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a6OwyXxpxRv",
        "outputId": "e4770138-4ae4-47da-e731-ade89468dca2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasa/core/tracker_store.py:1044: MovedIn20Warning: \u001b[31mDeprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. \u001b[32mTo prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". \u001b[36mSet environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message.\u001b[0m (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base: DeclarativeMeta = declarative_base()\n",
            "usage: rasa [-h] [--version]\n",
            "            {init,run,shell,train,interactive,telemetry,test,visualize,data,export,x,evaluate} ...\n",
            "rasa: error: unrecognized arguments: --project rasa_project\n",
            "Failed to create 'rasa_project' directory. Please check for errors in the RASA initialization step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlu_content = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "nlu:\n",
        "- intent: greet\n",
        "  examples: |\n",
        "    - hey\n",
        "    - hello\n",
        "    - hi\n",
        "    - good morning\n",
        "    - good evening\n",
        "    - hey there\n",
        "\n",
        "- intent: goodbye\n",
        "  examples: |\n",
        "    - bye\n",
        "    - goodbye\n",
        "    - see you around\n",
        "    - see you later\n",
        "\n",
        "- intent: affirm\n",
        "  examples: |\n",
        "    - yes\n",
        "    - indeed\n",
        "    - of course\n",
        "    - that sounds good\n",
        "    - correct\n",
        "\n",
        "- intent: deny\n",
        "  examples: |\n",
        "    - no\n",
        "    - never\n",
        "    - I don't think so\n",
        "    - don't like that\n",
        "    - no way\n",
        "    - not really\n",
        "\n",
        "- intent: mood_great\n",
        "  examples: |\n",
        "    - perfect\n",
        "    - great\n",
        "    - amazing\n",
        "    - feeling like a king\n",
        "    - wonderful\n",
        "    - I am feeling very good\n",
        "    - I am great\n",
        "    - I'm good\n",
        "\n",
        "- intent: mood_unhappy\n",
        "  examples: |\n",
        "    - sad\n",
        "    - very sad\n",
        "    - unhappy\n",
        "    - bad\n",
        "    - I don't feel very well\n",
        "    - I am sad\n",
        "    - I am depressed\n",
        "    - I'm not feeling very well\n",
        "\n",
        "- intent: bot_challenge\n",
        "  examples: |\n",
        "    - are you a bot?\n",
        "    - are you a human?\n",
        "    - am I talking to a bot?\n",
        "    - am I talking to a human?\n",
        "\n",
        "- intent: get_weather\n",
        "  examples: |\n",
        "    - What's the weather like today?\n",
        "    - Tell me the weather forecast.\n",
        "    - How's the weather?\n",
        "    - Is it going to rain today?\n",
        "    - Do I need an umbrella today?\n",
        "    - What's the temperature outside?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "os.makedirs(\"rasa_project/data\", exist_ok=True)\n",
        "\n",
        "\n",
        "with open(\"rasa_project/data/nlu.yml\", \"w\") as file:\n",
        "    file.write(nlu_content)\n",
        "    print(\"'nlu.yml' file created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6s1Mn-gp1e7",
        "outputId": "f7966c75-8616-4da6-911a-2e5c6efa0283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'nlu.yml' file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "domain_content = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "intents:\n",
        "  - greet\n",
        "  - goodbye\n",
        "  - affirm\n",
        "  - deny\n",
        "  - mood_great\n",
        "  - mood_unhappy\n",
        "  - bot_challenge\n",
        "  - get_weather\n",
        "\n",
        "responses:\n",
        "  utter_greet:\n",
        "    - text: \"Hey! How are you?\"\n",
        "\n",
        "  utter_cheer_up:\n",
        "    - text: \"Here is something to cheer you up!\"\n",
        "      image: \"https://i.imgur.com/nGF1K8f.jpg\"\n",
        "\n",
        "  utter_did_that_help:\n",
        "    - text: \"Did that help you?\"\n",
        "\n",
        "  utter_goodbye:\n",
        "    - text: \"Bye! Take care.\"\n",
        "\n",
        "  utter_happy:\n",
        "    - text: \"Great, carry on!\"\n",
        "\n",
        "  utter_bot_challenge:\n",
        "    - text: \"I am a bot, powered by Rasa.\"\n",
        "\n",
        "  utter_weather:\n",
        "    - text: \"The weather today is sunny with a high of 25 degrees Celsius.\"\n",
        "\n",
        "  utter_iamabot:\n",
        "    - text: \"I am a bot, powered by Rasa.\"\n",
        "\n",
        "session_config:\n",
        "  session_expiration_time: 60\n",
        "  carry_over_slots_to_new_session: true\n",
        "\"\"\"\n",
        "\n",
        "# Write to domain.yml\n",
        "with open(\"rasa_project/domain.yml\", \"w\") as file:\n",
        "    file.write(domain_content)\n",
        "    print(\"'domain.yml' file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZQVjQAp4W1",
        "outputId": "c225fd63-bea1-4e6e-d864-e0102fbe677c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'domain.yml' file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stories_content = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "stories:\n",
        "- story: greet user\n",
        "  steps:\n",
        "  - intent: greet\n",
        "  - action: utter_greet\n",
        "\n",
        "- story: say goodbye\n",
        "  steps:\n",
        "  - intent: goodbye\n",
        "  - action: utter_goodbye\n",
        "\n",
        "- story: get weather\n",
        "  steps:\n",
        "  - intent: get_weather\n",
        "  - action: utter_weather\n",
        "\n",
        "- story: bot challenge\n",
        "  steps:\n",
        "  - intent: bot_challenge\n",
        "  - action: utter_iamabot\n",
        "\"\"\"\n",
        "\n",
        "# Write to stories.yml\n",
        "with open(\"rasa_project/data/stories.yml\", \"w\") as file:\n",
        "    file.write(stories_content)\n",
        "    print(\"'stories.yml' file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHQ-erYNp6-g",
        "outputId": "0f8280e2-2e8d-45af-92bd-db9a3c56a101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'stories.yml' file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rules_content = \"\"\"\n",
        "version: \"3.1\"\n",
        "\n",
        "rules:\n",
        "- rule: Fallback rule\n",
        "  steps:\n",
        "  - intent: nlu_fallback\n",
        "  - action: utter_iamabot\n",
        "\"\"\"\n",
        "\n",
        "# Write to rules.yml\n",
        "with open(\"rasa_project/data/rules.yml\", \"w\") as file:\n",
        "    file.write(rules_content)\n",
        "    print(\"'rules.yml' file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OndRhBtOp9wD",
        "outputId": "80a8011d-93e0-4460-fb3a-0dc7424e63c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'rules.yml' file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_content = \"\"\"\n",
        "language: en\n",
        "version: \"3.1\"\n",
        "\n",
        "pipeline:\n",
        "  - name: WhitespaceTokenizer\n",
        "  - name: RegexFeaturizer\n",
        "  - name: LexicalSyntacticFeaturizer\n",
        "  - name: CountVectorsFeaturizer\n",
        "  - name: DIETClassifier\n",
        "    epochs: 100\n",
        "  - name: EntitySynonymMapper\n",
        "  - name: ResponseSelector\n",
        "    epochs: 100\n",
        "\n",
        "policies:\n",
        "  - name: MemoizationPolicy\n",
        "  - name: TEDPolicy\n",
        "    max_history: 5\n",
        "    epochs: 100\n",
        "  - name: RulePolicy\n",
        "\"\"\"\n",
        "\n",
        "# Write to config.yml\n",
        "with open(\"rasa_project/config.yml\", \"w\") as file:\n",
        "    file.write(config_content)\n",
        "    print(\"'config.yml' file created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Smqle-xqBe3",
        "outputId": "a266e260-f202-403a-d4ac-8c7f65b331b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'config.yml' file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# navigate to the RASA project directory\n",
        "os.chdir(\"rasa_project\")\n",
        "\n",
        "# train the RASA model\n",
        "!rasa train --quiet\n",
        "\n",
        "# verify that the model is trained and saved\n",
        "if os.path.exists(\"models\"):\n",
        "    print(\"RASA model trained and saved in the 'models' directory.\")\n",
        "else:\n",
        "    print(\"RASA model training failed. Please check the previous steps for errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCF0Qv6IqEGi",
        "outputId": "e02c7207-d1e6-49aa-cb8c-21c0acc9b8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/rasa/core/tracker_store.py:1044: MovedIn20Warning: \u001b[31mDeprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. \u001b[32mTo prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". \u001b[36mSet environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message.\u001b[0m (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base: DeclarativeMeta = declarative_base()\n",
            "/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/validation.py:134: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:52: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The intent 'affirm' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The intent 'deny' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The intent 'mood_great' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The intent 'mood_unhappy' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The utterance 'utter_did_that_help' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The utterance 'utter_bot_challenge' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The utterance 'utter_happy' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/shared/utils/io.py:99: UserWarning: The utterance 'utter_cheer_up' is not used in any story or rule.\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/engine/recipes/recipe.py:35: FutureWarning: From Rasa Open Source 4.0.0 onwards it will be required to specify a recipe in your model configuration. Defaulting to recipe 'default.v1'.\n",
            "  rasa.shared.utils.io.raise_deprecation_warning(\n",
            "\u001b[0m\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/utils/train_utils.py:530: UserWarning: constrain_similarities is set to `False`. It is recommended to set it to `True` when using cross-entropy loss.\n",
            "  rasa.shared.utils.io.raise_warning(\n",
            "\u001b[0m\u001b[92mYour Rasa model is trained and saved at 'models/20241126-181119-rust-alternator.tar.gz'.\u001b[0m\n",
            "RASA model trained and saved in the 'models' directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the pre-trained DialoGPT-large model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
      ],
      "metadata": {
        "id": "eIjjcDeyqGvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from rasa.core.agent import Agent\n",
        "from rasa.shared.core.trackers import DialogueStateTracker\n",
        "\n",
        "# load the trained RASA model\n",
        "agent = Agent.load(\"models\")\n",
        "\n",
        "async def get_rasa_response(user_input):\n",
        "    \"\"\"\n",
        "    Sends a user message to the RASA Agent and returns the response.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        response (str): The response generated by RASA.\n",
        "    \"\"\"\n",
        "    responses = await agent.handle_text(user_input, sender_id=\"bot_interaction\")\n",
        "\n",
        "    if responses:\n",
        "        return responses[0]['text']\n",
        "    else:\n",
        "        return \"I'm not sure how to respond to that.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x0-IcqiqPqb",
        "outputId": "60bbdffa-211d-4217-9dc1-5fe1cff342c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/random.py:370: DeprecationWarning: non-integer arguments to randrange() have been deprecated since Python 3.10 and will be removed in a subsequent version\n",
            "  return self.randrange(a, b+1)\n",
            "WARNING:tensorflow:5 out of the last 13 calls to <bound method RasaModel.predict_step of <rasa.nlu.classifiers.diet_classifier.DIET object at 0x7f63c71e39a0>> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\u001b[93m/usr/local/lib/python3.10/dist-packages/rasa/utils/train_utils.py:530: UserWarning: constrain_similarities is set to `False`. It is recommended to set it to `True` when using cross-entropy loss.\n",
            "  rasa.shared.utils.io.raise_warning(\n",
            "\u001b[0m<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "import torch\n",
        "\n",
        "class StopSequenceCriteria(StoppingCriteria):\n",
        "    def __init__(self, stop_sequence_ids):\n",
        "        \"\"\"\n",
        "        Initializes the stopping criteria with the token IDs of the stop sequence.\n",
        "\n",
        "        Args:\n",
        "            stop_sequence_ids (list): List of token IDs representing the stop sequence.\n",
        "        \"\"\"\n",
        "        self.stop_sequence_ids = stop_sequence_ids\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        \"\"\"\n",
        "        Determines whether to stop generation based on the stop sequence.\n",
        "\n",
        "        Args:\n",
        "            input_ids (torch.Tensor): The sequence of generated token IDs.\n",
        "            scores (torch.Tensor): The scores for the next token predictions.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the stop sequence is detected; False otherwise.\n",
        "        \"\"\"\n",
        "        # check if the last tokens match the stop sequence\n",
        "        if input_ids.shape[-1] < len(self.stop_sequence_ids):\n",
        "            return False\n",
        "\n",
        "        # extract the last tokens equal to the length of the stop sequence\n",
        "        last_tokens = input_ids[0, -len(self.stop_sequence_ids):].tolist()\n",
        "\n",
        "        return last_tokens == self.stop_sequence_ids\n",
        "\n",
        "def generate_dialo_response(\n",
        "    user_input,\n",
        "    chat_history_ids=None,\n",
        "    do_sample=True,\n",
        "    num_beams=1,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7,\n",
        "    no_repeat_ngram_size=3,\n",
        "    max_length=1000,\n",
        "    stop_sequence=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a response to the user input using DialoGPT with customizable parameters.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "        chat_history_ids (torch.Tensor, optional): The history of the conversation.\n",
        "        do_sample (bool): Whether to use sampling; otherwise, greedy decoding.\n",
        "        num_beams (int): Number of beams for beam search.\n",
        "        top_k (int): The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
        "        top_p (float): The cumulative probability for nucleus sampling.\n",
        "        temperature (float): Temperature value for scaling logits.\n",
        "        no_repeat_ngram_size (int): Prevent repeating n-grams of this size.\n",
        "        max_length (int): The maximum length of the generated response.\n",
        "        stop_sequence (str, optional): A phrase that, if generated, will stop further generation.\n",
        "\n",
        "    Returns:\n",
        "        response (str): The generated response from the model.\n",
        "        chat_history_ids (torch.Tensor): Updated chat history including the latest user input and model response.\n",
        "    \"\"\"\n",
        "\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "\n",
        "    if chat_history_ids is not None:\n",
        "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "\n",
        "    stopping_criteria = None\n",
        "    if stop_sequence:\n",
        "        stop_sequence_ids = tokenizer.encode(stop_sequence, add_special_tokens=False)\n",
        "        stopping_criteria = StoppingCriteriaList([StopSequenceCriteria(stop_sequence_ids)])\n",
        "\n",
        "\n",
        "    chat_history_ids = model.generate(\n",
        "        bot_input_ids,\n",
        "        max_length=max_length,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        do_sample=do_sample,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        num_beams=num_beams,\n",
        "        stopping_criteria=stopping_criteria\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return response, chat_history_ids"
      ],
      "metadata": {
        "id": "UPxE3NyrrYG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def chat_between_rasa_dialo():\n",
        "    \"\"\"\n",
        "    Facilitates a conversation between RASA and DialoGPT chatbots.\n",
        "    \"\"\"\n",
        "\n",
        "    user_message = \"Hello!\"\n",
        "\n",
        "\n",
        "    continue_chat = True\n",
        "\n",
        "\n",
        "    max_turns = 10\n",
        "    current_turn = 0\n",
        "\n",
        "\n",
        "    STOP_SEQUENCE = \"Goodbye!\"\n",
        "\n",
        "\n",
        "    dialo_chat_history = None\n",
        "\n",
        "    while continue_chat and current_turn < max_turns:\n",
        "        print(f\"\\n--- Turn {current_turn + 1} ---\")\n",
        "\n",
        "\n",
        "        dialo_response, dialo_chat_history = generate_dialo_response(\n",
        "            user_input=user_message,\n",
        "            chat_history_ids=dialo_chat_history,\n",
        "            do_sample=True,           # Sampling enabled for variability\n",
        "            num_beams=1,              # Beam search disabled\n",
        "            top_k=50,\n",
        "            top_p=0.95,\n",
        "            temperature=0.7,\n",
        "            no_repeat_ngram_size=3,\n",
        "            max_length=1000,\n",
        "            stop_sequence=STOP_SEQUENCE\n",
        "        )\n",
        "\n",
        "        print(f\"DialoGPT: {dialo_response}\")\n",
        "\n",
        "\n",
        "        if STOP_SEQUENCE.lower() in dialo_response.lower():\n",
        "            print(\"DialoGPT has ended the conversation.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        rasa_response = await get_rasa_response(dialo_response)\n",
        "        print(f\"RASA: {rasa_response}\")\n",
        "\n",
        "\n",
        "        if rasa_response.strip().lower() in ['goodbye', 'bye', 'exit']:\n",
        "            print(\"RASA has ended the conversation.\")\n",
        "            break\n",
        "\n",
        "\n",
        "        user_message = rasa_response\n",
        "        current_turn += 1\n",
        "\n",
        "    print(\"\\nConversation ended.\")"
      ],
      "metadata": {
        "id": "LBjcRc5QrcV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the interaction loop\n",
        "await chat_between_rasa_dialo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEHY7GE8rgW_",
        "outputId": "191b1530-6b7c-439a-8a76-aa47b3a0ad3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/asyncio/tasks.py:232: DeprecationWarning: `run_cell_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  result = coro.send(None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Turn 1 ---\n",
            "DialoGPT: Hi!\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.log          action_name=action_session_start rasa_events=[<rasa.shared.core.events.SessionStarted object at 0x7f643ac6a530>, ActionExecuted(action: action_listen, policy: None, confidence: None)]\n",
            "2024-11-26 18:13:57 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 18:13:57 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'greet', 'confidence': 0.9964724779129028} parse_data_text=Hi!\n",
            "2024-11-26 18:13:57 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:13:57 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:13:57 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user text: Hi! | previous action name: action_listen\n",
            "2024-11-26 18:13:57 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643aa162c0>]\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.log          action_name=utter_greet rasa_events=[BotUttered('Hey! How are you?', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_greet\"}, 1732644837.700923)]\n",
            "2024-11-26 18:13:57 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}]\n",
            "2024-11-26 18:13:57 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:13:57 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: Hey! How are you?\n",
            "\n",
            "--- Turn 2 ---\n",
            "DialoGPT: I am doing great!\n",
            "2024-11-26 18:14:00 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'mood_great', 'confidence': 0.4796372056007385} parse_data_text=I am doing great!\n",
            "2024-11-26 18:14:00 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:00 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'mood_great'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:00 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user text: I am doing great! | previous action name: action_listen\n",
            "2024-11-26 18:14:00 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_great | previous action name: action_listen\n",
            "2024-11-26 18:14:00 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643ac6ac20>]\n",
            "2024-11-26 18:14:00 [debug    ] processor.actions.log          action_name=action_default_fallback rasa_events=[<rasa.shared.core.events.UserUtteranceReverted object at 0x7f63c7a5ec80>]\n",
            "2024-11-26 18:14:00 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 18:14:00 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}]\n",
            "2024-11-26 18:14:00 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 18:14:00 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:00 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I'm not sure how to respond to that.\n",
            "\n",
            "--- Turn 3 ---\n",
            "DialoGPT: I know exactly how you feel.\n",
            "2024-11-26 18:14:04 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'mood_unhappy', 'confidence': 0.40018749237060547} parse_data_text=I know exactly how you feel.\n",
            "2024-11-26 18:14:04 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:04 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'mood_unhappy'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:04 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user text: I know exactly how you feel. | previous action name: action_listen\n",
            "2024-11-26 18:14:04 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: mood_unhappy | previous action name: action_listen\n",
            "2024-11-26 18:14:04 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f63c7a5eaa0>]\n",
            "2024-11-26 18:14:04 [debug    ] processor.actions.log          action_name=action_default_fallback rasa_events=[<rasa.shared.core.events.UserUtteranceReverted object at 0x7f63c7a5e980>]\n",
            "2024-11-26 18:14:04 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 18:14:04 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}]\n",
            "2024-11-26 18:14:04 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "2024-11-26 18:14:04 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:04 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I'm not sure how to respond to that.\n",
            "\n",
            "--- Turn 4 ---\n",
            "DialoGPT: Hi, I am you.\n",
            "2024-11-26 18:14:08 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'goodbye', 'confidence': 0.3252181112766266} parse_data_text=Hi, I am you.\n",
            "2024-11-26 18:14:08 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:08 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:08 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user text: Hi, I am you. | previous action name: action_listen\n",
            "2024-11-26 18:14:08 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "2024-11-26 18:14:08 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f64391d1ed0>]\n",
            "2024-11-26 18:14:08 [debug    ] processor.actions.log          action_name=utter_goodbye rasa_events=[BotUttered('Bye! Take care.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_goodbye\"}, 1732644848.1897001)]\n",
            "2024-11-26 18:14:08 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}]\n",
            "2024-11-26 18:14:08 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "2024-11-26 18:14:08 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:08 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: Bye! Take care.\n",
            "\n",
            "--- Turn 5 ---\n",
            "DialoGPT: You are me.\n",
            "2024-11-26 18:14:11 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'bot_challenge', 'confidence': 0.6948065757751465} parse_data_text=You are me.\n",
            "2024-11-26 18:14:11 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:11 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:11 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user text: You are me. | previous action name: action_listen\n",
            "2024-11-26 18:14:11 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 18:14:11 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f63c7a5ed10>]\n",
            "2024-11-26 18:14:11 [debug    ] processor.actions.log          action_name=utter_iamabot rasa_events=[BotUttered('I am a bot, powered by Rasa.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_iamabot\"}, 1732644851.145907)]\n",
            "2024-11-26 18:14:11 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}]\n",
            "2024-11-26 18:14:11 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "2024-11-26 18:14:11 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:11 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I am a bot, powered by Rasa.\n",
            "\n",
            "--- Turn 6 ---\n",
            "DialoGPT: Hello, I'm a bot, powered by rasa.\n",
            "2024-11-26 18:14:17 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'bot_challenge', 'confidence': 0.8834019303321838} parse_data_text=Hello, I'm a bot, powered by rasa.\n",
            "2024-11-26 18:14:17 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:17 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user text: Hello, I'm a bot, powered by rasa. | previous action name: action_listen\n",
            "2024-11-26 18:14:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 18:14:17 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f63c7a5f310>]\n",
            "2024-11-26 18:14:17 [debug    ] processor.actions.log          action_name=utter_iamabot rasa_events=[BotUttered('I am a bot, powered by Rasa.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_iamabot\"}, 1732644857.4380846)]\n",
            "2024-11-26 18:14:17 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}]\n",
            "2024-11-26 18:14:17 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "2024-11-26 18:14:17 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:17 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I am a bot, powered by Rasa.\n",
            "\n",
            "--- Turn 7 ---\n",
            "DialoGPT: What's the point?\n",
            "2024-11-26 18:14:22 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'get_weather', 'confidence': 0.9995104074478149} parse_data_text=What's the point?\n",
            "2024-11-26 18:14:22 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:22 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user text: What's the point? | previous action name: action_listen\n",
            "2024-11-26 18:14:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "2024-11-26 18:14:22 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f63c7a5ed70>]\n",
            "2024-11-26 18:14:22 [debug    ] processor.actions.log          action_name=utter_weather rasa_events=[BotUttered('The weather today is sunny with a high of 25 degrees Celsius.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_weather\"}, 1732644862.5425246)]\n",
            "2024-11-26 18:14:22 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}]\n",
            "2024-11-26 18:14:22 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "2024-11-26 18:14:22 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:22 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: The weather today is sunny with a high of 25 degrees Celsius.\n",
            "\n",
            "--- Turn 8 ---\n",
            "DialoGPT: 10C\n",
            "2024-11-26 18:14:26 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'mood_great', 'confidence': 0.2217945009469986} parse_data_text=10C\n",
            "2024-11-26 18:14:26 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:26 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'mood_great'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:26 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user text: 10C | previous action name: action_listen\n",
            "2024-11-26 18:14:26 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: mood_great | previous action name: action_listen\n",
            "2024-11-26 18:14:26 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643ac688b0>]\n",
            "2024-11-26 18:14:26 [debug    ] processor.actions.log          action_name=action_default_fallback rasa_events=[<rasa.shared.core.events.UserUtteranceReverted object at 0x7f643ac6a9e0>]\n",
            "2024-11-26 18:14:26 [debug    ] processor.slots.log            slot_values=\tsession_started_metadata: None\n",
            "2024-11-26 18:14:26 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}]\n",
            "2024-11-26 18:14:26 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "2024-11-26 18:14:26 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:26 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I'm not sure how to respond to that.\n",
            "\n",
            "--- Turn 9 ---\n",
            "DialoGPT: A!\n",
            "2024-11-26 18:14:31 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'bot_challenge', 'confidence': 0.5879541635513306} parse_data_text=A!\n",
            "2024-11-26 18:14:31 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:31 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:31 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user text: A! | previous action name: action_listen\n",
            "2024-11-26 18:14:31 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: bot_challenge | previous action name: action_listen\n",
            "2024-11-26 18:14:31 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f63c7a5f520>]\n",
            "2024-11-26 18:14:31 [debug    ] processor.actions.log          action_name=utter_iamabot rasa_events=[BotUttered('I am a bot, powered by Rasa.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_iamabot\"}, 1732644871.8843527)]\n",
            "2024-11-26 18:14:31 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}]\n",
            "2024-11-26 18:14:31 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 12] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "2024-11-26 18:14:31 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:31 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: I am a bot, powered by Rasa.\n",
            "\n",
            "--- Turn 10 ---\n",
            "DialoGPT: Is a.b\n",
            "2024-11-26 18:14:37 [debug    ] processor.message.parse        parse_data_entities=[] parse_data_intent={'name': 'get_weather', 'confidence': 0.4083382487297058} parse_data_text=Is a.b\n",
            "2024-11-26 18:14:37 [debug    ] processor.extract.slots        action_extract_slot=action_extract_slots len_extraction_events=0 rasa_events=[]\n",
            "2024-11-26 18:14:37 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}]\n",
            "2024-11-26 18:14:37 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 12] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 13] user text: Is a.b | previous action name: action_listen\n",
            "2024-11-26 18:14:37 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 12] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 13] user intent: get_weather | previous action name: action_listen\n",
            "2024-11-26 18:14:37 [debug    ] processor.actions.policy_prediction prediction_events=[<rasa.shared.core.events.DefinePrevUserUtteredFeaturization object at 0x7f643ac6a530>]\n",
            "2024-11-26 18:14:37 [debug    ] processor.actions.log          action_name=utter_weather rasa_events=[BotUttered('The weather today is sunny with a high of 25 degrees Celsius.', {\"elements\": null, \"quick_replies\": null, \"buttons\": null, \"attachment\": null, \"image\": null, \"custom\": null}, {\"utter_action\": \"utter_weather\"}, 1732644877.4411056)]\n",
            "2024-11-26 18:14:37 [debug    ] memoization.predict.actions    tracker_states=[{}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'greet'}, 'prev_action': {'action_name': 'utter_greet'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'goodbye'}, 'prev_action': {'action_name': 'utter_goodbye'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'bot_challenge'}, 'prev_action': {'action_name': 'utter_iamabot'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'action_listen'}}, {'user': {'intent': 'get_weather'}, 'prev_action': {'action_name': 'utter_weather'}}]\n",
            "2024-11-26 18:14:37 [debug    ] rule_policy.actions.find       current_states=\n",
            "[state 1] user intent: greet | previous action name: action_listen\n",
            "[state 2] user intent: greet | previous action name: utter_greet\n",
            "[state 3] user intent: goodbye | previous action name: action_listen\n",
            "[state 4] user intent: goodbye | previous action name: utter_goodbye\n",
            "[state 5] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 6] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 7] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 8] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 9] user intent: get_weather | previous action name: action_listen\n",
            "[state 10] user intent: get_weather | previous action name: utter_weather\n",
            "[state 11] user intent: bot_challenge | previous action name: action_listen\n",
            "[state 12] user intent: bot_challenge | previous action name: utter_iamabot\n",
            "[state 13] user intent: get_weather | previous action name: action_listen\n",
            "[state 14] user intent: get_weather | previous action name: utter_weather\n",
            "2024-11-26 18:14:37 [debug    ] processor.actions.policy_prediction prediction_events=[]\n",
            "2024-11-26 18:14:37 [debug    ] processor.actions.log          action_name=action_listen rasa_events=[]\n",
            "RASA: The weather today is sunny with a high of 25 degrees Celsius.\n",
            "\n",
            "Conversation ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPl82lLOWwO"
      },
      "source": [
        "## **LLM conversational skills**\n",
        "\n",
        "I've Created a chatbot using an LLM model. Create and manage a list of messages to keep track of the conversation. Use the keywords \"system\", \"user\" and \"assistant\" to assign a role to each message. More examples can be found [here](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "YCYW3WBs-_MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "FoQ1I-3PX-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]\n",
        "\n",
        "def generate_response(conversation, model, tokenizer, max_length=2048, temperature=0.7, top_p=0.9, stop_sequences=[\"\\nuser:\", \"\\nassistant:\"]):\n",
        "    \"\"\"\n",
        "    Generates a response from the assistant based on the conversation history.\n",
        "\n",
        "    Args:\n",
        "        conversation (list): List of messages with roles.\n",
        "        model: The pre-trained language model.\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        max_length (int): Maximum length of the generated response.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling threshold.\n",
        "        stop_sequences (list): Sequences where generation should stop.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\"\n",
        "    for message in conversation:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            prompt += f\"{message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            prompt += f\"user: {message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            prompt += f\"assistant: {message['content']}\\n\"\n",
        "\n",
        "    prompt += \"assistant: \"  # Indicate that the assistant should respond next\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[1] + 200,  # Allow up to 200 tokens for the response\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    assistant_response = output_text[len(prompt):].strip()\n",
        "\n",
        "\n",
        "    for stop in stop_sequences:\n",
        "        if stop in assistant_response:\n",
        "            assistant_response = assistant_response.split(stop)[0].strip()\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "def chat():\n",
        "    print(\"Chatbot initialized. Type 'exit', 'quit', or 'bye' to end the conversation.\\n\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "\n",
        "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
        "            print(\"Assistant: Goodbye! Have a great day!\")\n",
        "            break\n",
        "\n",
        "\n",
        "        conversation.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "\n",
        "        assistant_response = generate_response(conversation, model, tokenizer)\n",
        "\n",
        "\n",
        "        conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "        print(f\"Assistant: {assistant_response}\\n\")\n",
        "\n",
        "# Start the chat\n",
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpoRdacltD13",
        "outputId": "d8a2846d-4dba-4d53-c566-e3ce72c1159d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot initialized. Type 'exit', 'quit', or 'bye' to end the conversation.\n",
            "\n",
            "You: hello\n",
            "Assistant: urn\n",
            "\n",
            "You: how is it going?\n",
            "Assistant: urnategothedog.comuser : how are you doing?\n",
            "\n",
            "You: exit\n",
            "Assistant: Goodbye! Have a great day!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(conversation, model, tokenizer, max_length=2048, temperature=0.7, top_p=0.9, stop_sequences=[\"\\nuser:\", \"\\nassistant:\"], max_history=10):\n",
        "    \"\"\"\n",
        "    Generates a response from the assistant based on the conversation history.\n",
        "\n",
        "    Args:\n",
        "        conversation (list): List of messages with roles.\n",
        "        model: The pre-trained language model.\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        max_length (int): Maximum length of the generated response.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling threshold.\n",
        "        stop_sequences (list): Sequences where generation should stop.\n",
        "        max_history (int): Maximum number of past messages to retain.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "    # retain only the last 'max_history' messages plus the system prompt\n",
        "    system_message = [msg for msg in conversation if msg[\"role\"] == \"system\"]\n",
        "    user_and_assistant = conversation[-max_history:]\n",
        "    conversation_trimmed = system_message + user_and_assistant\n",
        "\n",
        "    # format the conversation into a single prompt string\n",
        "    prompt = \"\"\n",
        "    for message in conversation_trimmed:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            prompt += f\"{message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            prompt += f\"user: {message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            prompt += f\"assistant: {message['content']}\\n\"\n",
        "\n",
        "    prompt += \"assistant: \"\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=input_ids.shape[1] + 200,  # Allow up to 200 tokens for the response\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    assistant_response = output_text[len(prompt):].strip()\n",
        "\n",
        "\n",
        "    for stop in stop_sequences:\n",
        "        if stop in assistant_response:\n",
        "            assistant_response = assistant_response.split(stop)[0].strip()\n",
        "\n",
        "    return assistant_response"
      ],
      "metadata": {
        "id": "6hIZF6TStFjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"assistant: Hello! How can I assist you today?\\nuser: What's the weather like today?\\nassistant:\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n"
      ],
      "metadata": {
        "id": "RagqoUJctIBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt = \"Hello! How are you today?\"\n",
        "\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "\n",
        "temperature = 0.7\n",
        "top_p = 0.9\n",
        "\n",
        "\n",
        "output_ids = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=200,\n",
        "    temperature=temperature,\n",
        "    top_p=top_p,\n",
        "    do_sample=True,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "\n",
        "assistant_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Assistant: {assistant_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-eee3QctJrs",
        "outputId": "28a54961-b8af-4d97-c619-374424713694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: Hello! How are you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
        "]\n",
        "\n",
        "def generate_response(conversation, model, tokenizer, max_length=2048, temperature=0.7, top_p=0.9, stop_sequences=[\"\\nuser:\", \"\\nassistant:\"]):\n",
        "    \"\"\"\n",
        "    Generates a response from the assistant based on the conversation history.\n",
        "\n",
        "    Args:\n",
        "        conversation (list): List of messages with roles.\n",
        "        model: The pre-trained language model.\n",
        "        tokenizer: Corresponding tokenizer.\n",
        "        max_length (int): Maximum length of the generated response.\n",
        "        temperature (float): Sampling temperature.\n",
        "        top_p (float): Nucleus sampling threshold.\n",
        "        stop_sequences (list): Sequences where generation should stop.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated response.\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = \"\"\n",
        "    for message in conversation:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            prompt += f\"{message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            prompt += f\"user: {message['content']}\\n\"\n",
        "        elif message[\"role\"] == \"assistant\":\n",
        "            prompt += f\"assistant: {message['content']}\\n\"\n",
        "\n",
        "    prompt += \"assistant: \"  # Indicate that the assistant should respond next\n",
        "\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    assistant_response = output_text[len(prompt):].strip()\n",
        "    for stop in stop_sequences:\n",
        "        if stop in assistant_response:\n",
        "            assistant_response = assistant_response.split(stop)[0].strip()\n",
        "\n",
        "    return assistant_response\n",
        "\n",
        "user_message = \"Can you tell me a joke?\"\n",
        "\n",
        "conversation.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "assistant_response = generate_response(conversation, model, tokenizer)\n",
        "\n",
        "conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "print(f\"Assistant: {assistant_response}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTZh_KFvvMjD",
        "outputId": "3b11896b-96fb-420a-8d6d-12b6b95b0be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: urns\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "a88d8d275dc8276b143b02757b297c7b7ccc4199bd118b2f9ce33906ca7c97c0"
      }
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f06ba9a062a343bb8373b95cf53d3452": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d1d183e5afe413fbddc4308bf758c98",
              "IPY_MODEL_ae233e1fb2cf45b78345fb1ce82624a0",
              "IPY_MODEL_50a70c7034034b2ca2cf3a006fd9fc25"
            ],
            "layout": "IPY_MODEL_42c15e5e07484887a813afd0f2f9a4d7"
          }
        },
        "0d1d183e5afe413fbddc4308bf758c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9614cea6d0284a03aa7bc813434c99d8",
            "placeholder": "​",
            "style": "IPY_MODEL_564186891028403283c7f29501883831",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "ae233e1fb2cf45b78345fb1ce82624a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0acd84f2d6e04d5bb6d0f290d2420c4d",
            "max": 614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f86f80fe2414609b763bcccd2c5dc3e",
            "value": 614
          }
        },
        "50a70c7034034b2ca2cf3a006fd9fc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f290c903376480cb3561a66fd597aff",
            "placeholder": "​",
            "style": "IPY_MODEL_116e256d832c46e796122fa090b6f0f4",
            "value": " 614/614 [00:00&lt;00:00, 19.2kB/s]"
          }
        },
        "42c15e5e07484887a813afd0f2f9a4d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9614cea6d0284a03aa7bc813434c99d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564186891028403283c7f29501883831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0acd84f2d6e04d5bb6d0f290d2420c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f86f80fe2414609b763bcccd2c5dc3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f290c903376480cb3561a66fd597aff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116e256d832c46e796122fa090b6f0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e59ecb652d9e4deba9b51a7229c67bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0fa552ad45d442f1b1a758e217116a08",
              "IPY_MODEL_5f09b35065ba4e0093f3dac3c40de56b",
              "IPY_MODEL_35aa9950e189457487192d226442ff34"
            ],
            "layout": "IPY_MODEL_af45cee6512d485d8fe5b830783f107d"
          }
        },
        "0fa552ad45d442f1b1a758e217116a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e620c21e0c804021b5c8f3ebbf6ef9c1",
            "placeholder": "​",
            "style": "IPY_MODEL_dd51b5dd43ab478eb065c7ce653dbfb6",
            "value": "vocab.json: 100%"
          }
        },
        "5f09b35065ba4e0093f3dac3c40de56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d71e83251bc45ff95fd7d9d254804a9",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c6c09c9d302493c83d3f96f6c8fe281",
            "value": 1042301
          }
        },
        "35aa9950e189457487192d226442ff34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab3f7696e1824371a67b2fa72b15d537",
            "placeholder": "​",
            "style": "IPY_MODEL_730c388213674bc5b7f03dd4848f6d39",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 6.76MB/s]"
          }
        },
        "af45cee6512d485d8fe5b830783f107d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e620c21e0c804021b5c8f3ebbf6ef9c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd51b5dd43ab478eb065c7ce653dbfb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d71e83251bc45ff95fd7d9d254804a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c6c09c9d302493c83d3f96f6c8fe281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ab3f7696e1824371a67b2fa72b15d537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "730c388213674bc5b7f03dd4848f6d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2470044f68ca4abebf57b06210457a2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8a12548a35ae498c804fe90e9878b4ec",
              "IPY_MODEL_87500b89e1a34eba955dbfe1486e1766",
              "IPY_MODEL_7bfc411e63fd4c19b84c347731bae0ed"
            ],
            "layout": "IPY_MODEL_db640df184e34ca0b3fc5b92c531fe80"
          }
        },
        "8a12548a35ae498c804fe90e9878b4ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd366d7a5b3a41738e14c6ecd7931fd8",
            "placeholder": "​",
            "style": "IPY_MODEL_c68c2fcd5f18493698f93de87765cf0e",
            "value": "merges.txt: 100%"
          }
        },
        "87500b89e1a34eba955dbfe1486e1766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90329dd8ff07417a996ec3c89f667432",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a804c677de384bd0a364d2e5fc0bad92",
            "value": 456318
          }
        },
        "7bfc411e63fd4c19b84c347731bae0ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8c95ed550574e9da7c94e900e39c184",
            "placeholder": "​",
            "style": "IPY_MODEL_0628c31bc2fe4075bd445b0d842eacc8",
            "value": " 456k/456k [00:00&lt;00:00, 5.96MB/s]"
          }
        },
        "db640df184e34ca0b3fc5b92c531fe80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd366d7a5b3a41738e14c6ecd7931fd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c68c2fcd5f18493698f93de87765cf0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90329dd8ff07417a996ec3c89f667432": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a804c677de384bd0a364d2e5fc0bad92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8c95ed550574e9da7c94e900e39c184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0628c31bc2fe4075bd445b0d842eacc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf3de6caed00477582782ad832c1a6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f69eb1a1ce71470ab03dfecc2f1d35d3",
              "IPY_MODEL_e07b5cbe48fb4b5d85c9f4113544172b",
              "IPY_MODEL_56acb681a2e046cb81bfc433425d9fa2"
            ],
            "layout": "IPY_MODEL_330fdcb967144027942d78848400d0eb"
          }
        },
        "f69eb1a1ce71470ab03dfecc2f1d35d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004553eb3c9d45d28029c0655240f618",
            "placeholder": "​",
            "style": "IPY_MODEL_07829a4504fe4a5c84771228de4a0c13",
            "value": "config.json: 100%"
          }
        },
        "e07b5cbe48fb4b5d85c9f4113544172b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_490d5c4cd7a84643848568b29f80920d",
            "max": 642,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b38e54ca60d94b2ea040f315f6f63154",
            "value": 642
          }
        },
        "56acb681a2e046cb81bfc433425d9fa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_450910e7d66540fab8cbbbd1c6d82d1c",
            "placeholder": "​",
            "style": "IPY_MODEL_1ef5f03bf1dc4258832da43f79838376",
            "value": " 642/642 [00:00&lt;00:00, 30.0kB/s]"
          }
        },
        "330fdcb967144027942d78848400d0eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "004553eb3c9d45d28029c0655240f618": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07829a4504fe4a5c84771228de4a0c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "490d5c4cd7a84643848568b29f80920d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b38e54ca60d94b2ea040f315f6f63154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "450910e7d66540fab8cbbbd1c6d82d1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ef5f03bf1dc4258832da43f79838376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9abb166e28e742eb95ed5ba12947d751": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5da2541399ce4b429eac088a0eb9e749",
              "IPY_MODEL_b884a897dbf044c396680a153c28708d",
              "IPY_MODEL_0f4a740efdd7476fa3622cccbb26e5b1"
            ],
            "layout": "IPY_MODEL_b8911881fb9e465190112ef3fac38db0"
          }
        },
        "5da2541399ce4b429eac088a0eb9e749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8035771c30df4f1bb90cd5feea219feb",
            "placeholder": "​",
            "style": "IPY_MODEL_2780128fc92a4e5484f26784401f01d2",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "b884a897dbf044c396680a153c28708d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6a9690f6d743d8b3c0ef1e9c2bc078",
            "max": 1752292117,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_452eb7ec76a3490686521cf2ae4089aa",
            "value": 1752292117
          }
        },
        "0f4a740efdd7476fa3622cccbb26e5b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce4fbf40196b4c14ac8f406efd18838f",
            "placeholder": "​",
            "style": "IPY_MODEL_e356fa19988b43be8de48db99f3a1053",
            "value": " 1.75G/1.75G [00:10&lt;00:00, 39.2MB/s]"
          }
        },
        "b8911881fb9e465190112ef3fac38db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8035771c30df4f1bb90cd5feea219feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2780128fc92a4e5484f26784401f01d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf6a9690f6d743d8b3c0ef1e9c2bc078": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "452eb7ec76a3490686521cf2ae4089aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ce4fbf40196b4c14ac8f406efd18838f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e356fa19988b43be8de48db99f3a1053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa6efd57fff040eeb8e918389af4199e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_882a9a37105e4ff4b32493b6856a12af",
              "IPY_MODEL_568176484cda471da3f4978001e0e6af",
              "IPY_MODEL_6a894bbbb63748d08abc8803ba832cf1"
            ],
            "layout": "IPY_MODEL_e7891f898e164d55adecaa2e8de39d1a"
          }
        },
        "882a9a37105e4ff4b32493b6856a12af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bd5f11de2c344e2961689e0e4503378",
            "placeholder": "​",
            "style": "IPY_MODEL_4c5bc227971b426bb61f2219cace91d6",
            "value": "model.safetensors: 100%"
          }
        },
        "568176484cda471da3f4978001e0e6af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c540be215d3f4bb28280d2f2ebe1ab81",
            "max": 1752264928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f08d01fc5fae480d99821692461cd78f",
            "value": 1752264928
          }
        },
        "6a894bbbb63748d08abc8803ba832cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed61fd8f52b94fd38a8a15104335d697",
            "placeholder": "​",
            "style": "IPY_MODEL_d0ad408b7f834477a1cd6139dfe74d9f",
            "value": " 1.75G/1.75G [00:23&lt;00:00, 105MB/s]"
          }
        },
        "e7891f898e164d55adecaa2e8de39d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd5f11de2c344e2961689e0e4503378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5bc227971b426bb61f2219cace91d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c540be215d3f4bb28280d2f2ebe1ab81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f08d01fc5fae480d99821692461cd78f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed61fd8f52b94fd38a8a15104335d697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0ad408b7f834477a1cd6139dfe74d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bb619be2eff4c55ba9c864ceff5ed33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35991b2da4cd45559243c6e8b1405ce6",
              "IPY_MODEL_422588c2cc91476eb186459196503b3e",
              "IPY_MODEL_dd357dd22dc54d788578cf34f645368a"
            ],
            "layout": "IPY_MODEL_5d71f786a23843eea32a54aa8f6d88a5"
          }
        },
        "35991b2da4cd45559243c6e8b1405ce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d67c6616e9a94bbcacfb55a31ce61280",
            "placeholder": "​",
            "style": "IPY_MODEL_6672ac26d0d646399c9df69a87a26ebb",
            "value": "generation_config.json: 100%"
          }
        },
        "422588c2cc91476eb186459196503b3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aad1307ed3a545d9a63e5145f362b6b8",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b37c3e7705043a4b93b70ed5ad34511",
            "value": 124
          }
        },
        "dd357dd22dc54d788578cf34f645368a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f19d9903f934a05829fedd69aa7472c",
            "placeholder": "​",
            "style": "IPY_MODEL_133c431a9378467fa27baf47dd283291",
            "value": " 124/124 [00:00&lt;00:00, 3.10kB/s]"
          }
        },
        "5d71f786a23843eea32a54aa8f6d88a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d67c6616e9a94bbcacfb55a31ce61280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6672ac26d0d646399c9df69a87a26ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aad1307ed3a545d9a63e5145f362b6b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b37c3e7705043a4b93b70ed5ad34511": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f19d9903f934a05829fedd69aa7472c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "133c431a9378467fa27baf47dd283291": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
